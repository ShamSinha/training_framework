# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/metrics/02_sub_level_metrics.ipynb.

# %% auto 0
__all__ = ['texture_mapping', 'get_all_file_locs', 'cuboid_volume', 'cuboid_volume_subset', 'volume_callback', 'convert2df',
           'compute_metrics_from_cache', 'texture_subset', 'texture_callback', 'get_scan_stats', 'get_dataset_stats',
           'get_dataset_summary']

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 3
import torch
import pandas as pd
import numpy as np
from pathlib import Path, PosixPath
from typing import List, Union, Callable
from tqdm import tqdm 


from .det_metrics import DetMetrics, assign_tp_fp_fn_linear_assignment


# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 5
def get_all_file_locs(root: Union[str, PosixPath], read_dir: Union[str, List[str]]) -> List[Union[str, PosixPath]]:
    # Convert list of pt files to AnnotImg format.
    if isinstance(read_dir, str):
        files = list((Path(root) / read_dir).glob("*.pt"))
    elif isinstance(read_dir, list):
        files = [j for i in read_dir for j in (Path(root) / i).glob("*.pt")]
    else:
        raise NotImplementedError("only List of dirs or a dir is accepted")
    return files

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 13
def cuboid_volume(boxes): return (boxes[:, 3:] - boxes[:, :3]).prod(1)

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 20
def cuboid_volume_subset(vol, bins, index=0):
    _bin = bins[index]
    return (vol>_bin[0]) & (vol<=_bin[1])

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 21
def volume_callback(x, bins, index=0): 
    keep_in = cuboid_volume_subset(np.asarray(x["img_in"]["volume"]), bins, index=index)
    x["img_in"]["boxes"] = x["img_in"]["boxes"][keep_in]
    return x["img_in"], x["img_out"]

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 26
def convert2df(metrics):
    df = pd.DataFrame(metrics)
    for i in df.columns:
        df[i] = df[i].apply(lambda x: round(x, 2) if isinstance(x, (int, float)) else [round(i, 2) for i in x])
    return df[["conf", "iou", "FROC", "AP", "recall", "precision", "tp", "fp", "fn", "AP_interp", "FROC_interp", "FROC_thresholds", "avg_tp_iou"]]

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 28
def compute_metrics_from_cache(meters: List[DetMetrics], files: Union[str, List[Union[str, PosixPath]]], callbacks: Union[Callable, List[Callable]] = None):
    # where callbacks is a function or a list of functions to apply in sequence
    files = files if isinstance(files, list) else Path(files).glob("*.pt")
    [i.reset() for i in meters]
    for file in tqdm(files):
        x = torch.load(file)
        img, img_out = x["img_in"], x["img_out"]
        if callbacks is not None:
            if isinstance(callbacks, list):
                for callback in callbacks:
                    x["img_in"], x["img_out"] = img, img_out
                    img, img_out = callback(x)
            else:
                img, img_out = callbacks(x)
        for meter in meters:
            meter.update(img_out["boxes"], img_out["scores"], img["boxes"])
    metrics = [i.compute() for i in meters]
    return convert2df(metrics)

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 32
texture_mapping = {
    "Non-Solid/GGO": "texture_non_solid",
    "non-Solid/Mixed": "texture_non_solid",
    "Non-Solid/Mixed": "texture_non_solid",
    "non solid": "texture_non_solid",
    "Solid": "texture_solid",
    "solid": "texture_solid",
    "Part Solid/Mixed": "texture_part_solid",
    "Solid/Mixed": "texture_part_solid",
    "part solid": "texture_part_solid",
    "texture_part_solid": "texture_part_solid", 
    "texture_non_solid": "texture_non_solid", 
    "texture_solid": "texture_solid",
}

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 33
def texture_subset(textures, target_texture='solid'):
    return textures == target_texture

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 34
def texture_callback(x, texture): 
    textures = np.vectorize(texture_mapping.get)(np.asarray(x["img_in"]["texture"]))
    keep_in = texture_subset(textures, target_texture=np.asarray(texture_mapping[texture]))
    x["img_in"]["boxes"] = x["img_in"]["boxes"][keep_in]
    return x["img_in"], x["img_out"]

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 59
def get_scan_stats(pred_bbox: np.ndarray, pred_scores: np.ndarray, gt_bbox: np.ndarray, conf_thr: float, iou_thr: float):
    keep = pred_scores >= conf_thr
    pred_bbox = pred_bbox[keep]
    pred_scores = pred_scores[keep]
    tp, fp, fn, tp_iou = assign_tp_fp_fn_linear_assignment(pred_bbox, gt_bbox, iou_thr=iou_thr)
    return tp, fp, fn, tp_iou

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 64
def get_dataset_stats(files, conf_thr=0.9, iou_thr=0.1) -> pd.DataFrame:
    stats = {}
    # files is a list of paths of cached .pt files with input and output bboxes, output scores and input texture and volume info
    for file in files:
        ds = torch.load(file)
        num_nodules = ds["img_in"]["boxes"].shape[0]
        tp, fp, fn, tp_iou = get_scan_stats(pred_bbox=ds["img_out"]["boxes"], pred_scores=ds["img_out"]["scores"], gt_bbox=ds["img_in"]["boxes"], conf_thr=conf_thr, iou_thr=iou_thr)
        sensitivity = np.cumsum(tp)/(sum(tp)+sum(fn))
        stats[file.stem] = {
            "num_nodules": num_nodules,
            "tp": np.sum(tp),
            "fp": np.sum(fp),
            "fn": np.sum(fn),
            "recall": sensitivity.max() if sensitivity.size else 0,
            "avg_tp_iou": 0 if not tp_iou.size else np.mean(tp_iou, axis=0)
        }
    df = pd.DataFrame.from_dict(stats, orient='index')
    df.index.name = 'sid'
    df.reset_index(inplace=True)
    return df

# %% ../../nbs/metrics/02_sub_level_metrics.ipynb 65
def get_dataset_summary(dirs, root, conf_thr=0.9, iou_thr=0.1):
    summary = {}
    for dataset_name in tqdm(dirs):
        if isinstance(dataset_name, list): continue
        files = get_all_file_locs(root=root, read_dir=dataset_name)
        df = get_dataset_stats(files, conf_thr=conf_thr, iou_thr=iou_thr)
        summary[dataset_name] = {}
        summary[dataset_name]['num_scans'] = df.shape[0]
        summary[dataset_name]['num_nodules'] = df["num_nodules"].sum()
    return summary
