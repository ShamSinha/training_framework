# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/networks/08_vitdet3d.ipynb.

# %% auto 0
__all__ = ['conv3d_reduce', 'SimpleFeaturePyramidNetwork', 'VitDet3dBackbonewithFPN3D']

# %% ../../nbs/networks/08_vitdet3d.ipynb 2
import torch
import fastcore.all as fc
import torch.nn as nn

from medct.vitdet3d import VitDet3dBackbone, VitDetConfig, VitDet3dLayerNorm
from collections import OrderedDict

# %% ../../nbs/networks/08_vitdet3d.ipynb 22
def conv3d_reduce(in_dim, out_dim):
    layers = [
                        nn.Conv3d(in_dim, out_dim, kernel_size=2, stride=2),
                        VitDet3dLayerNorm(out_dim), 
                        nn.GELU()
                ]
    return nn.Sequential(*layers)

# %% ../../nbs/networks/08_vitdet3d.ipynb 23
# copied from https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py
class SimpleFeaturePyramidNetwork(torch.nn.Module):
    def __init__(self, dim, out_channels, scales):
        super().__init__()
        self.scales = sorted(scales)[::-1]
        ##Scales should be always from high to low 
        for n, scale in enumerate(self.scales):
            if scale not in [2, 1, 0.5, 0.25]: raise NotImplementedError("These modules are not implemented.")
            if scale == 2:
                out_dim = dim // 2 
                layers = [nn.ConvTranspose3d(dim, dim // 2, kernel_size=2, stride=2)]
            
            if scale == 1:
                out_dim = dim
                layers = []
            
            if scale == 0.5:
                out_dim = dim 
                layers = [conv3d_reduce(out_dim, out_dim)] 
                
            if scale == 0.25:
                out_dim = dim
                layers = [conv3d_reduce(out_dim, out_dim), 
                          conv3d_reduce(out_dim, out_dim)]      
                
                
            
            layers.extend([
                nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), 
                VitDet3dLayerNorm(out_channels), 
                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None),
                VitDet3dLayerNorm(out_channels), 
            ])
            
            layers = nn.Sequential(*layers)
            self.add_module(f"layer{n+1}", layers)
    
    def forward(self, x):
        out = OrderedDict()
        for n, _ in enumerate(self.scales):
            out[f"layer{n+1}"] = getattr(self, f"layer{n+1}")(x)
        return out    

# %% ../../nbs/networks/08_vitdet3d.ipynb 28
class VitDet3dBackbonewithFPN3D(nn.Module):
    def __init__(self, backbone_cfg, scales=[2, 1, 0.5, 0.25], out_channels=256):
        super().__init__()
        fc.store_attr(names=["backbone_cfg", "scales", "out_channels"])
        from omegaconf import DictConfig, OmegaConf #during inference self.backbone_cfg is DictConfig which is not supported by transformers.
        if isinstance(self.backbone_cfg, DictConfig):
            self.backbone_cfg = OmegaConf.to_object(self.backbone_cfg)
        self.cfg = VitDetConfig(**self.backbone_cfg)
        self.body = VitDet3dBackbone(self.cfg)
        self.fpn = SimpleFeaturePyramidNetwork(self.cfg.hidden_size, out_channels, scales)
        
    def forward(self, x):
        out = self.body(x).feature_maps[-1]
        y = self.fpn(out)
        return y
