{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521df6ed",
   "metadata": {},
   "source": [
    "## we will implement vitdet3d with FPN here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285afad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp networks/vitdet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055b714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch\n",
    "import fastcore.all as fc\n",
    "import torch.nn as nn\n",
    "\n",
    "from medct.vitdet3d import VitDet3dBackbone, VitDetConfig, VitDet3dLayerNorm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203de379",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VitDetConfig(image_size=(96, 192, 192), \n",
    "                      patch_size=(4, 8, 8), \n",
    "                      hidden_size=96,\n",
    "                      num_channels=1,\n",
    "                      use_relative_position_embeddings=True, \n",
    "                      window_block_indices=list(range(4)),\n",
    "                      window_size =(4, 4, 4), \n",
    "                      out_indices = [2, 4], \n",
    "                      num_hidden_layers= 4,\n",
    "                      out_features = [\"stage2\", \"stage4\"], \n",
    "                      stage_names = [\"stem\"]+[f\"stage{i}\" for i in range(1, 5)])\n",
    "model = VitDet3dBackbone(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fe7b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDet3dBackbone(\n",
       "  (embeddings): ViTDet3dEmbeddings(\n",
       "    (projection): Conv3d(1, 96, kernel_size=(4, 8, 8), stride=(4, 8, 8))\n",
       "  )\n",
       "  (encoder): VitDet3dEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-3): 4 x VitDet3dLayer(\n",
       "        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): VitDet3dAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): VitDet3dMlp(\n",
       "          (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (act): GELUActivation()\n",
       "          (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e83061",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 96, 192, 192))\n",
    "out = model(x, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057c660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 24, 24, 24]), torch.Size([1, 96, 24, 24, 24])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.feature_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199d2795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24]),\n",
       " torch.Size([1, 96, 24, 24, 24])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in out.hidden_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654e673",
   "metadata": {},
   "source": [
    "## Simple FPN3D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fecef",
   "metadata": {},
   "source": [
    "> we got a stride of (4, 8, 8) from the backbone. Now we need to \n",
    "- downsample this to 12 and 6\n",
    "- keep it the same as 24\n",
    "- upsample it to 48\n",
    "\n",
    "so that we have 4 layers to process through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2712bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = [0.25, 0.5, 1, 2]\n",
    "dim = 96\n",
    "out_channels = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e54a5",
   "metadata": {},
   "source": [
    "## Scale it to 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd616d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "  (1): Conv3d(48, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "  (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = dim // 2 \n",
    "layers = [\n",
    "    nn.ConvTranspose3d(dim, dim // 2, kernel_size=2, stride=2), \n",
    "    nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), \n",
    "    nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None)\n",
    "]\n",
    "layers = nn.Sequential(*layers)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8dff2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 48, 48, 48])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers(out.feature_maps[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b24149",
   "metadata": {},
   "source": [
    "## Scale to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f533a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 24, 24, 24])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = dim\n",
    "layers = [\n",
    "    nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), \n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "    nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None),\n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "]\n",
    "layers = nn.Sequential(*layers)\n",
    "layers(out.feature_maps[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e74512",
   "metadata": {},
   "source": [
    "## Scale to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5752a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 12, 12, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = dim\n",
    "layers = [\n",
    "    nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "    nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), \n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "    nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None),\n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "]\n",
    "layers = nn.Sequential(*layers)\n",
    "layers(out.feature_maps[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d2cc382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 12, 12, 12])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = dim\n",
    "layers = [\n",
    "    nn.Conv3d(out_dim, out_dim, kernel_size=2, stride=2),\n",
    "    VitDet3dLayerNorm(out_dim), \n",
    "    nn.GELU(),\n",
    "    nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), \n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "    nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None),\n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "]\n",
    "layers = nn.Sequential(*layers)\n",
    "layers(out.feature_maps[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfde4a",
   "metadata": {},
   "source": [
    "## Scale to 0.25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232731bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 6, 6, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = dim\n",
    "layers = [\n",
    "    nn.Conv3d(out_dim, out_dim, kernel_size=2, stride=2),\n",
    "    VitDet3dLayerNorm(out_dim), \n",
    "    nn.GELU(),\n",
    "    nn.Conv3d(out_dim, out_dim, kernel_size=2, stride=2),\n",
    "    VitDet3dLayerNorm(out_dim), \n",
    "    nn.GELU(),\n",
    "    nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), \n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "    nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None),\n",
    "    VitDet3dLayerNorm(out_channels), \n",
    "]\n",
    "layers = nn.Sequential(*layers)\n",
    "layers(out.feature_maps[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257e440",
   "metadata": {},
   "source": [
    "## Combine everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d0d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def conv3d_reduce(in_dim, out_dim):\n",
    "    layers = [\n",
    "                        nn.Conv3d(in_dim, out_dim, kernel_size=2, stride=2),\n",
    "                        VitDet3dLayerNorm(out_dim), \n",
    "                        nn.GELU()\n",
    "                ]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a867380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "# copied from https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py\n",
    "class SimpleFeaturePyramidNetwork(torch.nn.Module):\n",
    "    def __init__(self, dim, out_channels, scales):\n",
    "        super().__init__()\n",
    "        self.scales = sorted(scales)[::-1]\n",
    "        ##Scales should be always from high to low \n",
    "        for n, scale in enumerate(self.scales):\n",
    "            if scale not in [2, 1, 0.5, 0.25]: raise NotImplementedError(\"These modules are not implemented.\")\n",
    "            if scale == 2:\n",
    "                out_dim = dim // 2 \n",
    "                layers = [nn.ConvTranspose3d(dim, dim // 2, kernel_size=2, stride=2)]\n",
    "            \n",
    "            if scale == 1:\n",
    "                out_dim = dim\n",
    "                layers = []\n",
    "            \n",
    "            if scale == 0.5:\n",
    "                out_dim = dim \n",
    "                layers = [conv3d_reduce(out_dim, out_dim)] \n",
    "                \n",
    "            if scale == 0.25:\n",
    "                out_dim = dim\n",
    "                layers = [conv3d_reduce(out_dim, out_dim), \n",
    "                          conv3d_reduce(out_dim, out_dim)]      \n",
    "                \n",
    "                \n",
    "            \n",
    "            layers.extend([\n",
    "                nn.Conv3d(out_dim, out_channels, kernel_size=1, bias=None), \n",
    "                VitDet3dLayerNorm(out_channels), \n",
    "                nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1, bias=None),\n",
    "                VitDet3dLayerNorm(out_channels), \n",
    "            ])\n",
    "            \n",
    "            layers = nn.Sequential(*layers)\n",
    "            self.add_module(f\"layer{n+1}\", layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for n, _ in enumerate(self.scales):\n",
    "            out[f\"layer{n+1}\"] = getattr(self, f\"layer{n+1}\")(x)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43fadb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleFeaturePyramidNetwork(\n",
       "  (layer1): Sequential(\n",
       "    (0): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    (1): Conv3d(48, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (2): VitDet3dLayerNorm()\n",
       "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (4): VitDet3dLayerNorm()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv3d(96, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (1): VitDet3dLayerNorm()\n",
       "    (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (3): VitDet3dLayerNorm()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): VitDet3dLayerNorm()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Conv3d(96, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (2): VitDet3dLayerNorm()\n",
       "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (4): VitDet3dLayerNorm()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): VitDet3dLayerNorm()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): VitDet3dLayerNorm()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (2): Conv3d(96, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (3): VitDet3dLayerNorm()\n",
       "    (4): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (5): VitDet3dLayerNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfpn = SimpleFeaturePyramidNetwork(dim=96, out_channels=256, scales=[0.25, 0.5, 1, 2])\n",
    "sfpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a850f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_out = sfpn(out.feature_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1518e2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layer1', torch.Size([1, 256, 48, 48, 48])),\n",
       " ('layer2', torch.Size([1, 256, 24, 24, 24])),\n",
       " ('layer3', torch.Size([1, 256, 12, 12, 12])),\n",
       " ('layer4', torch.Size([1, 256, 6, 6, 6]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.shape) for k, v in fpn_out.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b03ec794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7426960"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for name, params in sfpn.named_parameters():\n",
    "    count+=params.numel()\n",
    "count #Around 7 million and this is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a68fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class VitDet3dBackbonewithFPN3D(nn.Module):\n",
    "    def __init__(self, backbone_cfg, scales=[2, 1, 0.5, 0.25], out_channels=256):\n",
    "        super().__init__()\n",
    "        fc.store_attr(names=[\"backbone_cfg\", \"scales\", \"out_channels\"])\n",
    "        from omegaconf import DictConfig, OmegaConf #during inference self.backbone_cfg is DictConfig which is not supported by transformers.\n",
    "        if isinstance(self.backbone_cfg, DictConfig):\n",
    "            self.backbone_cfg = OmegaConf.to_object(self.backbone_cfg)\n",
    "        self.cfg = VitDetConfig(**self.backbone_cfg)\n",
    "        self.body = VitDet3dBackbone(self.cfg)\n",
    "        self.fpn = SimpleFeaturePyramidNetwork(self.cfg.hidden_size, out_channels, scales)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x).feature_maps[-1]\n",
    "        y = self.fpn(out)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "351282cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VitDet3dBackbonewithFPN3D(\n",
       "  (body): VitDet3dBackbone(\n",
       "    (embeddings): ViTDet3dEmbeddings(\n",
       "      (projection): Conv3d(1, 96, kernel_size=(4, 8, 8), stride=(4, 8, 8))\n",
       "    )\n",
       "    (encoder): VitDet3dEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x VitDet3dLayer(\n",
       "          (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): VitDet3dAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): VitDet3dMlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fpn): SimpleFeaturePyramidNetwork(\n",
       "    (layer1): Sequential(\n",
       "      (0): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): Conv3d(48, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (2): VitDet3dLayerNorm()\n",
       "      (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): VitDet3dLayerNorm()\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv3d(96, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (1): VitDet3dLayerNorm()\n",
       "      (2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (3): VitDet3dLayerNorm()\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv3d(96, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        (1): VitDet3dLayerNorm()\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Conv3d(96, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (2): VitDet3dLayerNorm()\n",
       "      (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (4): VitDet3dLayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(image_size=(96, 192, 192), \n",
    "          patch_size=(4, 8, 8), \n",
    "          hidden_size=96,\n",
    "          num_channels=1,\n",
    "          use_relative_position_embeddings=True, \n",
    "          window_block_indices=list(range(4)),\n",
    "          window_size =(4, 4, 4), \n",
    "          out_indices = [2, 4], \n",
    "          num_hidden_layers= 4,\n",
    "          out_features = [\"stage2\", \"stage4\"], \n",
    "          stage_names = [\"stem\"]+[f\"stage{i}\" for i in range(1, 5)])\n",
    "model = VitDet3dBackbonewithFPN3D(config, scales=[2, 1, 0.5])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a21cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 96, 192, 192))\n",
    "fpn_out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7d167f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layer1', torch.Size([1, 256, 48, 48, 48])),\n",
       " ('layer2', torch.Size([1, 256, 24, 24, 24])),\n",
       " ('layer3', torch.Size([1, 256, 12, 12, 12]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.shape) for k, v in fpn_out.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "708efe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
