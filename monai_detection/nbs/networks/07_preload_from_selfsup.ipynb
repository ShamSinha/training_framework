{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856b32a4",
   "metadata": {},
   "source": [
    "## how to load network weights from selfsup to supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937a5ff",
   "metadata": {},
   "source": [
    "1) Load the supervised model \n",
    "2) copy the weights to the supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a6dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp networks/selfsup_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch \n",
    "from loguru import logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae198a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fastcore.all as fc\n",
    "\n",
    "from mmengine.config import Config\n",
    "from voxdet.utils import locate_cls\n",
    "from voxdet.networks.monai_retina3d import retina_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3f30ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile(\"../../configs/lidc/exp_2_convnextv2.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dab3835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading backbone from MedCT\n"
     ]
    }
   ],
   "source": [
    "model = retina_detector(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36186b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextV2Model3d(\n",
       "  (embeddings): ConvNextV2Embeddings3d(\n",
       "    (patch_embeddings): Conv3d(2, 40, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
       "    (layernorm): ConvNextV2LayerNorm3d()\n",
       "  )\n",
       "  (encoder): ConvNextV2Encoder3d(\n",
       "    (stages): ModuleList(\n",
       "      (0): ConvNextV2Stage3d(\n",
       "        (downsampling_layer): Identity()\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextV2Layer3d(\n",
       "            (dwconv): Conv3d(40, 40, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=40)\n",
       "            (layernorm): ConvNextV2LayerNorm3d()\n",
       "            (pwconv1): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (grn): ConvNextV2GRN3d()\n",
       "            (pwconv2): Linear(in_features=160, out_features=40, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextV2Layer3d(\n",
       "            (dwconv): Conv3d(40, 40, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=40)\n",
       "            (layernorm): ConvNextV2LayerNorm3d()\n",
       "            (pwconv1): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (grn): ConvNextV2GRN3d()\n",
       "            (pwconv2): Linear(in_features=160, out_features=40, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextV2Layer3d(\n",
       "            (dwconv): Conv3d(40, 40, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=40)\n",
       "            (layernorm): ConvNextV2LayerNorm3d()\n",
       "            (pwconv1): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (grn): ConvNextV2GRN3d()\n",
       "            (pwconv2): Linear(in_features=160, out_features=40, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ConvNextV2Stage3d(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextV2LayerNorm3d()\n",
       "          (1): Conv3d(40, 80, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextV2Layer3d(\n",
       "            (dwconv): Conv3d(80, 80, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=80)\n",
       "            (layernorm): ConvNextV2LayerNorm3d()\n",
       "            (pwconv1): Linear(in_features=80, out_features=320, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (grn): ConvNextV2GRN3d()\n",
       "            (pwconv2): Linear(in_features=320, out_features=80, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextV2Layer3d(\n",
       "            (dwconv): Conv3d(80, 80, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=80)\n",
       "            (layernorm): ConvNextV2LayerNorm3d()\n",
       "            (pwconv1): Linear(in_features=80, out_features=320, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (grn): ConvNextV2GRN3d()\n",
       "            (pwconv2): Linear(in_features=320, out_features=80, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextV2Layer3d(\n",
       "            (dwconv): Conv3d(80, 80, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=80)\n",
       "            (layernorm): ConvNextV2LayerNorm3d()\n",
       "            (pwconv1): Linear(in_features=80, out_features=320, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (grn): ConvNextV2GRN3d()\n",
       "            (pwconv2): Linear(in_features=320, out_features=80, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((80,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network.feature_extractor.body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fd73d",
   "metadata": {},
   "source": [
    "> Load the model weights from selfsup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06c965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters', 'cfg'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(\"../../resources/selfsup/exp1_epoch=744-step=168370-val_rloss=1.235.ckpt\")\n",
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cdb032b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#73) ['beddings.mask_token','beddings.patch_embeddings.weight','beddings.patch_embeddings.bias','beddings.layernorm.weight','beddings.layernorm.bias','ncoder.stages.0.layers.0.dwconv.weight','ncoder.stages.0.layers.0.dwconv.bias','ncoder.stages.0.layers.0.layernorm.weight','ncoder.stages.0.layers.0.layernorm.bias','ncoder.stages.0.layers.0.pwconv1.weight'...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = fc.L(weights[\"state_dict\"].keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cee29009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patch_embeddings.weight'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[1].split(\".\", 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee8c2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for key, value in weights[\"state_dict\"].items():\n",
    "    base, key = key.split(\".\", 1)\n",
    "    if base == \"beddings\": new_dict[\"embeddings.\"+key] = value\n",
    "    if base == \"ncoder\": new_dict[\"encoder.\"+key] = value\n",
    "    if base == \"ayernorm\": new_dict[\"layernorm.\"+key] = value\n",
    "    if base == \"coder\": new_dict[\"encoder.\"+key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bc1036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_dict, \"../../resources/selfsup/exp1_cleaned_epoch=744-step=168370-val_rloss=1.235.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a4c3e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#70) ['embeddings.patch_embeddings.weight','embeddings.patch_embeddings.bias','embeddings.layernorm.weight','embeddings.layernorm.bias','encoder.stages.0.layers.0.dwconv.weight','encoder.stages.0.layers.0.dwconv.bias','encoder.stages.0.layers.0.layernorm.weight','encoder.stages.0.layers.0.layernorm.bias','encoder.stages.0.layers.0.pwconv1.weight','encoder.stages.0.layers.0.pwconv1.bias'...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = model.network.feature_extractor.body.state_dict()\n",
    "network_keys = fc.L(model_dict.keys())\n",
    "network_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39721a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) ['encoder.stages.1.layers.2.grn.bias','encoder.stages.1.layers.2.pwconv2.weight','encoder.stages.1.layers.2.pwconv2.bias','layernorm.weight','layernorm.bias']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_keys[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0504cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.weight torch.Size([40, 2, 4, 8, 8]) torch.Size([40, 2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k in network_keys:\n",
    "    m = new_dict[k]\n",
    "    n = model_dict[k]\n",
    "    if m.shape == n.shape:continue \n",
    "    print(k, m.shape, n.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55b1e783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 2, 2, 4, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_pos_embed = torch.nn.functional.interpolate(\n",
    "            m, size=n.shape[2:], mode=\"trilinear\", align_corners=False\n",
    "        )\n",
    "patch_pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c1dcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = {}\n",
    "for k in network_keys:\n",
    "    m = new_dict[k]\n",
    "    n = model_dict[k]\n",
    "    if m.shape == n.shape:\n",
    "        final_weights[k] = m\n",
    "        continue\n",
    "    new_m = torch.nn.functional.interpolate(\n",
    "            m, size=n.shape[2:], mode=\"trilinear\", align_corners=False\n",
    "        )\n",
    "    final_weights[k] = new_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d424168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "model.network.feature_extractor.body.load_state_dict(final_weights)\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e9065",
   "metadata": {},
   "source": [
    "```\n",
    " ┃ Name                   ┃ Type                             ┃ Params ┃\n",
    "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
    "│ 0 │ model                  │ ConvNextV2ForMaskedImageModeling │  699 K │\n",
    "│ 1 │ model.model            │ ConvNextV2Model3d                │  367 K │\n",
    "│ 2 │ model.model.embeddings │ ConvNextV2Embeddings3d           │ 20.6 K │\n",
    "│ 3 │ model.model.encoder    │ ConvNextV2Encoder3d              │  347 K │\n",
    "│ 4 │ model.model.layernorm  │ LayerNorm                        │    160 │\n",
    "│ 5 │ model.decoder          │ Sequential                       │  331 K │\n",
    "│ 6 │ model.decoder.0        │ Conv3d                           │  331 K │\n",
    "│ 7 │ model.decoder.1        │ PixelShuffle3d                   │      0 │\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4dfa7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def load_from_selfup_retina(seflsup_weight_loc, model):\n",
    "    weights = torch.load(seflsup_weight_loc)\n",
    "    if \"state_dict\" in weights.keys(): weights = weights[\"state_dict\"]\n",
    "    \n",
    "    model_dict = model.network.feature_extractor.body.state_dict()\n",
    "    network_keys = model_dict.keys()\n",
    "    \n",
    "    final_weights = {}\n",
    "    for k in network_keys:\n",
    "        if k not in weights.keys():\n",
    "            logger.warn(f\"{k} not in weights\")\n",
    "        if k not in model_dict.keys():\n",
    "            logger.warn(f\"{k} not in model_dict\")\n",
    "        m = weights[k]\n",
    "        n = model_dict[k]\n",
    "        logger.info(f\"mapping weights {k}-{m.shape}\")\n",
    "        if m.shape == n.shape:\n",
    "            final_weights[k] = m\n",
    "            continue\n",
    "        logger.info(f\"resizing weights {k} from {m.shape} to {n.shape}\")\n",
    "        new_m = torch.nn.functional.interpolate(\n",
    "                m, size=n.shape[2:], mode=\"trilinear\", align_corners=False\n",
    "            )\n",
    "        final_weights[k] = new_m\n",
    "    return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afe48059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qct_deep)",
   "language": "python",
   "name": "qct_deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
