defaults:
  # - cls: cls_tb
  # - seg: seg_tb
  # - files: e2e
  # - model/optimizer: adam
  # - model/scheduler: one_cycle_lr
  - override hydra/hydra_logging: disabled #prevent logging
  - override hydra/job_logging: disabled #prevent logging
  - _self_

#prevent logging
hydra:
  output_subdir: null
  run:
    dir: .

path:
  checkpoint_dir: "/raid/qxr_ln_trainings/checkpoints"
  checkpoint_path: ""
  log_directory: "/raid/qxr_ln_trainings/logs"

cls:
    sampling_tags: ['nodule',solitary, "nota", "normal", "homogenous", "inhomogenous", "diffuse" ,  "regular_border" , "irregular_border" , "tinynodule", "calcified", "cancer",  "large"]
    heads: ['nodule',solitary, "normal", "homogenous", "inhomogenous", "diffuse" ,  "regular_border" , "irregular_border" , "tinynodule", "calcified", "cancer",  "large"]
    user_class_wts: {"nodule": 1, "solitary": 1, "nota": 2, "normal": 2, "homogenous": 0.25, "inhomogenous": 0.25, "diffuse": 0.25, "regular_border": 0.25, "irregular_border": 0.25, "tinynodule": 0.25, "calcified": 0.25, "cancer": 0.25, "large": 0.25}
    loss_wts : {"nodule": 2, "solitary": 2, "normal": 1, "homogenous": 2, "inhomogenous": 2, "diffuse": 2, "regular_border": 2, "irregular_border": 2, "tinynodule": 2, "calcified": 2, "cancer": 2, "large": 2}
    alpha: 1

seg:
    sampling_tags: ['nodule']
    heads: ['nodule']
    dice_threshold: [0.2]
    user_class_wts : {'nodule': 2.0, 'solitary': 2.0,'homogenous': 1.0, 'inhomogenous': 1.0, 'diffuse': 2.0, 'regular_border': 1.0, 'irregular_border': 1.0, 'tinynodule': 1.0, 'calcified': 1.0, 'cancer': 1.0, 'large': 1.0}
    loss_wts : {'nodule': 2.0, 'homogenous': 2.0, 'inhomogenous': 2.0, 'solitary': 2.0, 'diffuse': 2.0, 'regular_border': 2.0, 'irregular_border': 2.0, 'tinynodule': 2.0, 'calcified': 2.0, 'cancer': 2.0, 'large': 2.0}
    alpha: 1.0

files:   
  testing_csv: '/raid/qxr_ln_trainings/testing/nlst_jsrt.csv'
  testing_images:  ['/raid/cxr/qxr_ln_data/jsrt_nlst/data']
  testing_annotation_path: ['/raid/cxr/qxr_ln_data/jsrt_nlst/masks']  ##giving a dummy path for now to avoid error
  # testing_csv: "/raid/cxr/qxr_ln_data/LN_test/combined_test_csv_wo_padchest_09-09-24.csv"
  # testing_images:  ['/raid/cxr/qxr_ln_data/segmed/segmed_pngs/', '/raid/cxr/qxr_ln_data/sbri/pngs/', '/raid/cxr/qxr_ln_data/vrad_imgs/' , '/raid/cxr/qxr_ln_data/jsrt_nlst/data/', '/raid/cxr/qxr_ln_data/mgh/all_pngs/' , '/raid/cxr/qxr_ln_data/fda_data/nodule_fda/', '/raid/cxr_data/testing/images/']
  # testing_annotation_path: ['/raid/cxr_data/testing/test_annotations', '/raid/cxr/qxr_ln_data/fda_data/fda_masks', '/raid/cxr/qxr_ln_data/jsrt_nlst/masks']  ## doing seg analysis only on internal test - have to generate for bot h fda/nlstjsrt
  # testing_csv: '/fast_data_e2e11/qxr_ln_trainings/testing/29_aug_5000_sample_data.csv'
  # testing_images:  ['/models_common_e2e/cxr_data/training/images','/fast_data_e2e11/piyush/fake_data/fake_data_batch_3/fake_imgs', '/fast_data_e2e11/piyush/fake_data/fake_data_batch_4/fake_imgs']
  # testing_annotation_path: ['/models_common_e2e/cxr_data/training/annotations/lms_annotations'] ##giving a dummy path for now to avoid error
  test_data_type: nlst_jsrt
  # test_data_type: all_testset

model:
  __target__: cxr_training.nnmodule.nnmodule_controller.LitModel
  encoder_library: smp
  encoder: tu-tf_efficientnetv2_m_in21ft1k
  decoder: UnetPlusPlus
  in_channels: 1
  out_channels: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    params:
      pct_start: 0.3
      max_lr: 0.0005
      anneal_strategy: 'cos'
      div_factor: 250 #self.max_lr / self.optimizer_lr
      final_div_factor: 100 # self.optimizer_lr / self.final_lr
      base_momentum: 0.90 # momentum changes beta[0] value in adamw
      max_momentum: 0.95
      three_phase: False
  optimizer:
    _target_: torch.optim.AdamW
    params:
      lr: 0.0005
      betas: [0.9, 0.99]
      eps: 1e-08
      weight_decay: 0.1

trainer:
    description: "base_training"
    project: "qxr_ln_training"
    model_file: "7_sept_effnet_unetplusplus_old_data_manoj_lr_0.2_v1"
    batch_size: 64
    num_workers: 16
    gpus: [1]
    recipe: "testing"
    recipe_module: cxr_training.recipes.test_recipe.TestRecipe
    checkpoint_dir: "/raid/qxr_ln_trainings/checkpoints"
    model_list: ['model_1600-epoch=227-val_loss=3.758750.ckpt']

params:
    im_size: 1024
    f1_beta_value:
    inference_on_subset: False
    inference_on_images: False
    inference_type: "cls_seg_comb"
    metric_type: "cls_seg"
    inference_run: True
    metric_run: True

use_clearml: False
validate_config: False
