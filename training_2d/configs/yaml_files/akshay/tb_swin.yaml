defaults:
  - cls: cls_tb
  - seg: seg_tb
  - files: e2e
  - model/optimizer: adam
  - model/scheduler: one_cycle_lr
  - override hydra/hydra_logging: disabled #prevent logging
  - override hydra/job_logging: disabled #prevent logging
  - _self_

#prevent logging
hydra:
  output_subdir: null
  run:
    dir: .

path:
  checkpoint_dir: "/home/users/akshay.v/qxr_trainingcheckpoint/"
  checkpoint_path: ""
  log_directory: "/home/users/akshay.v/qxr_training/logs/"

cls:
  sampling_tags: ['pediatric_tuberculosis','normal', 'nota']
  heads: ['pediatric_tuberculosis','normal']
  user_class_wts: {'pediatric_tuberculosis': 2,'normal': 1, 'nota': 1}
  loss_wts: {'pediatric_tuberculosis': 2, 'normal': 1}
  alpha: 2

seg:
  sampling_tags: ['pediatric_tuberculosis']
  heads:  ['pediatric_tuberculosis']
  dice_threshold: [0.2]
  user_class_wts: { 'pediatric_tuberculosis': 2}
  loss_wts: { 'pediatric_tuberculosis': 2}
  alpha: 5

model:
  __target__: cxr_training.nnmodule.nnmodule_controller.LitModel
  encoder: swinv2_tiny_window8_256
  decoder: unet
  in_channels: 1
  out_channels: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    params:
      pct_start: 0.3
      max_lr: 0.0005
      anneal_strategy: 'cos'
      div_factor: 250 #self.max_lr / self.optimizer_lr
      final_div_factor: 100 # self.optimizer_lr / self.final_lr
      base_momentum: 0.90 # momentum changes beta[0] value in adamw
      max_momentum: 0.95
      three_phase: False

trainer:
  description: "test"
  project: "Pediatric_tb"
  model_file: "swinv2_tiny_unet_test"
  check_gradients: False
  recipe_module: cxr_training.recipes.base_recipe.BaseRecipe
  recipe: cls_seg
  total_epochs: 10
  accumulate_grad_batches: 32
  strategy: "ddp"
  batch_size: 1
  precision: 16
  num_workers: 8
  train_samples: 5120
  validation_samples: 5000
  gpus: [0]
  fast_dev_run: False

params:
  data_loader: cxr_training.data.dataloader.base_dataloader.DataModule
  dataset_type: cxr_training.data.datasets.cls_seg_dataset.Base_dataset
  metric_type: "default"
  im_size: 1024
  age_alpha: 0.01
  loss_type: "dice-bce"
  sources: []
  equal_source_sampling: False
  mask_threshold: 0.2
  sampler: "train_weighted_val_weighted"

use_clearml: False
validate_config: False