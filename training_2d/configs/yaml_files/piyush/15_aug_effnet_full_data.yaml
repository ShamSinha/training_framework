defaults:
  # - cls: cls_tb
  # - seg: seg_tb
  # - files: e2e
  # - model/optimizer: adam
  # - model/scheduler: one_cycle_lr
  - override hydra/hydra_logging: disabled #prevent logging
  - override hydra/job_logging: disabled #prevent logging
  - _self_

#prevent logging
hydra:
  output_subdir: null
  run:
    dir: .

path:
  checkpoint_dir: "/fast_data_e2e11/qxr_ln_trainings/checkpoints"
  checkpoint_path: ""
  log_directory: "/fast_data_e2e11/qxr_ln_trainings/logs"


cls: 
  sampling_tags: ['nodule', 'nipple_shadow', 'normal'] 
  heads: ['nodule', 'nipple_shadow', 'normal']
  user_class_wts: {'nodule': 2, 'nipple_shadow': 1, 'normal': 1}
  loss_wts : {'nodule': 2.18, 'nipple_shadow': 4.73, 'normal': 1.0}
  alpha: 2

seg:
  sampling_tags: ['nodule', 'nipple_shadow']
  heads: ['nodule', 'nipple_shadow']
  dice_threshold: [0.2, 0.2] 
  user_class_wts: {'nodule': 1.0, 'nipple_shadow': 1.0} 
  loss_wts: {'nodule': 1.0, 'nipple_shadow': 2.16}
  alpha: 5
  

files:   
  ground_truth_csv: '/fast_data_e2e11/piyush/training/training_csvs/train_data_8L_14_08_24.csv'
  img_folder_path:  ['/fast_data_e2e11/piyush/fake_data/fake_imgs_batch_2/', '/models_common_e2e/cxr_data/training/images/',
                     '/fast_data_e2e11/piyush/fake_data/fake_data_batch_3/fake_imgs']
  annotation_path: ['/fast_data_e2e11/piyush/fake_data/all_fake_masks/', '/models_common_e2e/cxr_data/training/annotations/',
                    '/fast_data_e2e11/piyush/fake_data/fake_data_batch_3/fake_masks']

model:
  __target__: cxr_training.nnmodule.nnmodule_controller.LitModel
  encoder_library: smp
  encoder: tu-tf_efficientnetv2_m
  decoder: DeepLabV3Plus
  in_channels: 1
  out_channels: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    params:
      pct_start: 0.3
      max_lr: 0.2
      anneal_strategy: 'cos'
      div_factor: 25 #self.max_lr / self.optimizer_lr
      steps_per_epoch: 1,  # int((epoch_size / (bsize * acc_size))),
      epochs: initial_stage_epochs
  optimizer:
    _target_: torch.optim.SGD
    params:
      lr: 0.2
      # betas: [0.9, 0.99]
      # eps: 1e-08
      weight_decay: 0.0001

trainer:
  description: "base_training"
  project: "qxr_ln_training"
  model_file: "15_aug_effnet_full_data"
  batch_size: 16
  num_workers: 4
  gpus: [0]
  recipe: "testing"
  recipe_module: cxr_training.recipes.test_recipe.TestRecipe
  checkpoint_dir: "/fast_data_e2e11/qxr_ln_trainings/checkpoints"
  model_list: ["model_1806-last.ckpt"]

params:
  data_loader: cxr_training.data.dataloader.base_dataloader.DataModule
  dataset_type: cxr_training.data.datasets.cls_seg_dataset.Base_dataset
  metric_type: "default"
  im_size: 1024
  # age_alpha: 0.01
  loss_type: "sq_dice-bce"
  sources: []
  equal_source_sampling: False
  mask_threshold: 0.2
  use_real_fake_sampling: True
  real_data_wts: 0.4
  sampler: "train_weighted_val_weighted"


use_clearml: True
validate_config: False
