defaults:
  # - cls: cls_tb
  # - seg: seg_tb
  # - files: e2e
  # - model/optimizer: adama
  # - model/scheduler: one_cycle_lr
  - override hydra/hydra_logging: disabled #prevent logging
  - override hydra/job_logging: disabled #prevent logging
  - _self_

#prevent logging
hydra:
  output_subdir: null
  run:
    dir: .

path:
  checkpoint_dir: "/raid/qxr_ln_trainings/checkpoints"
  checkpoint_path: ""
  log_directory: "/raid/qxr_ln_trainings/logs"


cls: 
  sampling_tags: ['nodule', 'regular_border', 'irregular_border', 'homogenous', 'inhomogenous', 'normal', 'nota'] 
  heads: ['nodule', 'regular_border', 'irregular_border', 'homogenous', 'inhomogenous', 'normal']
  user_class_wts:  {'nodule': 3.1254740435873964, 'regular_border': 4.120448567451853 , 'irregular_border': 5.3393107873337255, 'homogenous': 5.289269720598228 , 'inhomogenous': 5.30327400405999, 'normal': 1.0, 'nota': 1.0703267513043493}
  loss_wts : { 'nodule' : 3.570616706311101, 'regular_border' : 4.707299528704338 , 'irregular_border': 6.0997570388651505 , 'homogenous': 6.0425889208820385 , 'inhomogenous': 6.058587751071711 , 'normal': 1.0 }
  alpha: 2

seg:
  sampling_tags: ['nodule', 'regular_border', 'irregular_border', 'homogenous', 'inhomogenous']
  heads: ['nodule', 'regular_border', 'irregular_border', 'homogenous', 'inhomogenous']
  dice_threshold: [0.35, 0.35, 0.35, 0.35, 0.35] 
  user_class_wts: {'nodule': 1.0, 'inhomogenous': 2.8790972830850134, 'homogenous': 2.8639117736803104, 'irregular_border': 2.91835828188158,'regular_border': 1.7380297338765145}
  loss_wts: { 'nodule' : 1.0, 'regular_border' : 1.3183435568382738,  'irregular_border' : 1.7083203100696214,  'homogenous' : 1.692309597443406, 'inhomogenous': 1.6967902884572001}
  alpha: 5
  

files:   
  ground_truth_csv: '/raid/qxr_ln_training_data/training_csvs/31_aug_2M_with_3d_data_no_real_nodules.csv'
  # img_folder_path:  ['/models_common_e2e/cxr_data/training/images',
  #                    '/fast_data_e2e11/piyush/fake_data/fake_data_batch_3/fake_imgs',
  #                     '/fast_data_e2e11/piyush/fake_data/fake_data_batch_4/fake_imgs']
  # annotation_path: ['/models_common_e2e/cxr_data/training/annotations/lms_annotations',
  #                   '/fast_data_e2e11/piyush/fake_data/fake_data_batch_4/fake_masks',
  #                   '/fast_data_e2e11/piyush/fake_data/fake_data_batch_3/fake_masks']
  img_folder_path:  ['/raid/qxr_ln_training_data/cxr_data/training/images',
                     '/raid/qxr_ln_training_data/fake_data_2d/fake_imgs']
  annotation_path: ['/raid/qxr_ln_training_data/cxr_data/training/annotations/lms_annotations',
                    '/raid/qxr_ln_training_data/fake_data_2d/fake_masks']


model:
  __target__: cxr_training.nnmodule.nnmodule_controller.LitModel
  encoder_library: smp
  encoder: tu-tf_efficientnetv2_m
  decoder: unet
  in_channels: 1
  out_channels: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    params:
      pct_start: 0.3
      max_lr: 0.02
      anneal_strategy: cos
      div_factor: 25
      steps_per_epoch: 1,
      epochs: initial_stage_epochs
  optimizer:
    _target_: torch.optim.SGD
    params:
      lr: 0.02
      weight_decay: 0.0001


trainer:
  description: base_training
  project: qxr_ln_training
  model_file: 3_sep_effnet_fake_2d_data
  check_gradients: false
  recipe_module: cxr_training.recipes.base_recipe.BaseRecipe
  recipe: cls_seg
  total_epochs: 100
  accumulate_grad_batches: 20
  strategy: ddp
  batch_size: 12
  precision: 16
  num_workers: 2
  train_samples: 40000
  validation_samples: 10000
  gpus: [-1]
  fast_dev_run: false
params:
  data_loader: cxr_training.data.dataloader.base_dataloader.DataModule
  dataset_type: cxr_training.data.datasets.cls_seg_dataset.Base_dataset
  metric_type: default
  im_size: 1024
  loss_type: sq_dice-bce
  sources: []
  equal_source_sampling: false
  mask_threshold: 0.2
  use_real_fake_sampling: true
  real_data_wts: 0.4
  sampler: train_weighted_val_weighted


use_clearml: True
validate_config: False
