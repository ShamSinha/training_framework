# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_activations.ipynb.

# %% auto 0
__all__ = ['GeneralRelu']

# %% ../nbs/06_activations.ipynb 1
import torch.nn as nn
import torch.nn.functional as F 
import fastcore.all as fc

# %% ../nbs/06_activations.ipynb 3
class GeneralRelu(nn.Module):
    def __init__(self, leak=None, sub=None, maxv=None):
        super().__init__()
        fc.store_attr()

    def forward(self, x): 
        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)
        if self.sub is not None: x -= self.sub
        if self.maxv is not None: x.clamp_max_(self.maxv)
        return x
    
    def __repr__(self): return f"GeneralRelu: leak:{self.leak}-sub:{self.sub}-maxv:{self.maxv}"
