# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/networks/02_res_se_net.ipynb.

# %% auto 0
__all__ = ['act_gr', 'conv3d', 'ResBlock', 'ResStage', 'kaiming_init_weights', 'ResNet', 'resnet10', 'resnet18', 'resnet50']

# %% ../../nbs/networks/02_res_se_net.ipynb 2
import torch
import torch.nn as nn
import fastcore.all as fc

from functools import partial
from typing import List

from ..activations import GeneralRelu

# %% ../../nbs/networks/02_res_se_net.ipynb 13
act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)

# %% ../../nbs/networks/02_res_se_net.ipynb 15
def conv3d(ni, nf, ks=3, stride=2, act=None, norm=None, bias=False, padding=None, dilation=1):
    pad = (ks//2 if isinstance(ks, int) else tuple(k // 2 for k in ks)) if padding is None else padding
    res = nn.Conv3d(ni, nf, stride=stride, kernel_size=ks, padding=pad, bias=bias, dilation=dilation)
    act = nn.Identity() if act is None else act()
    norm = nn.Identity() if norm is None else norm(nf)
    return nn.Sequential(*[i for i in [res, norm, act] if not isinstance(i, nn.Identity)])

# %% ../../nbs/networks/02_res_se_net.ipynb 22
def _conv3d_block(ni, nf, stride, act=act_gr, norm=None, ks=3):
    return nn.Sequential(conv3d(ni, nf, stride=stride, act=act, norm=norm, ks=ks),
                         conv3d(nf, nf, stride=1, act=None, norm=norm, ks=ks))

# %% ../../nbs/networks/02_res_se_net.ipynb 29
def _conv3d_bottleneck(ni, nf, stride, exp=4, act=act_gr, norm=None, ks=3):
    return nn.Sequential(conv3d(ni, nf, stride=1, act=act, norm=norm, ks=1),
                         conv3d(nf, nf, stride=stride, act=act, norm=norm, ks=ks),
                         conv3d(nf, nf*exp, stride=1, act=None, norm=norm, ks=1))

# %% ../../nbs/networks/02_res_se_net.ipynb 38
class ResBlock(nn.Module):
    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=nn.BatchNorm3d, block_type="basic"):
        super().__init__()
        fc.store_attr()
        if self.block_type not in ["basic", "bottleneck"]: raise NotImplementedError(f"block_type: {self.block_type} missing")
        exp = 4 if block_type!="basic" else 1
        self.convs = _conv3d_block(ni, nf, stride, act=act, ks=ks, norm=norm) if block_type=="basic" else \
        _conv3d_bottleneck(ni, nf, stride, exp=4, act=act, norm=norm, ks=ks)
        self.downsample = fc.noop if ni==nf*exp else conv3d(ni, nf*exp, ks=1, stride=stride, norm=norm, act=None, bias=True)
        #self.pool = fc.noop #if stride==1 else nn.AvgPool2d(2, ceil_mode=True)
        self.act = act()

    def forward(self, x): return self.act(self.convs(x) + self.downsample(x)) #self.idconv(self.pool(x)))

# %% ../../nbs/networks/02_res_se_net.ipynb 46
class ResStage(nn.Module):
    def __init__(self, ni: int, ip: int, nf: int, layers: int, stride=1, ks=3, act=act_gr, norm=nn.BatchNorm3d, block_type="basic"):
        super().__init__()
        fc.store_attr()
        self.block0 = ResBlock(ni, ip, stride, ks, act, norm, block_type=block_type)
        for i in range(1, layers): 
            setattr(self, f"block{i}", ResBlock(nf, ip, 1, ks, act, norm, block_type=block_type))
    
    def forward(self, x): 
        for i in range(self.layers): x = getattr(self, f"block{i}")(x)
        return x 

# %% ../../nbs/networks/02_res_se_net.ipynb 57
def kaiming_init_weights(m):
    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight)
    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)

# %% ../../nbs/networks/02_res_se_net.ipynb 58
class ResNet(nn.Module):
    def __init__(self, \
                 ic: int, \
                 ip: List[int], \
                 layers: List[int], \
                 c1_ks = [7, 7, 7], \
                 c1_stride = [2, 2, 1], \
                 base_pool=False, \
                 norm=nn.BatchNorm3d, \
                 act=act_gr, \
                 block_type="basic", \
                 init_type="kaiming", \
                 dilated_conv_last_layer=False):
        """ic is input channels, c1_ks and c1_stride are conv1 kernel size and stride respectively\
        layers: How 
        block_planes: 
        """
        super().__init__()
        fc.store_attr()
        self.base = conv3d(self.ic, ip[0], ks=c1_ks, stride=c1_stride, act=act_gr, norm=nn.BatchNorm3d)
        if self.base_pool: self.pool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        _num = 0.5 if block_type=="basic" else 2
        #ks = 3 if block_type=="basic" else 1
        for n, (layer, _ip) in enumerate(zip(self.layers, self.ip)):
            if block_type =="basic": ni, nf = _ip if n==0 else int(_ip/2), _ip if n==0 else _ip*2
            else: ni, nf = _ip if n==0 else _ip*2, _ip*4 
#             ni = ip[n] if n==0 else int(ip[n]*_num)
#             nf = int(ni*_num*2) if n==0 else int(ni*_num)
            stride = 1 if n==0 else 2
            setattr(self, f"layer{n+1}", ResStage(ni, _ip, nf, layers=layer, stride=stride, \
                                                  ks=3, norm=norm, act=act, block_type=block_type))
        
        if self.dilated_conv_last_layer: self.dil_conv = conv3d(ip[-1], ip[-1], ks=3, stride=c1_stride, act=act_gr, norm=nn.BatchNorm3d, padding=2, dilation=2)
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        if init_type == "kaiming": self.apply(kaiming_init_weights)
        else: raise NotImplementedError("Only kaiming implmented")
    
    def forward(self, x):
        out = self.base(x)
        if self.base_pool: out = self.pool(out)
        for i in range(4): out = getattr(self, f"layer{i+1}")(out)
        if self.dilated_conv_last_layer: out = self.dil_conv(out)
        out = self.avg_pool(out)
        return out        

# %% ../../nbs/networks/02_res_se_net.ipynb 69
def resnet10(ic, c1_ks, c1_stride, base_pool=True, norm=nn.BatchNorm3d, act=act_gr, dilated_conv_last_layer=False):
    return ResNet(ic=ic, ip=[64, 128, 256, 512], layers=[1, 1, 1, 1], block_type="basic", \
                 c1_ks=c1_ks, c1_stride=c1_stride, norm=norm, act=act, base_pool=base_pool, dilated_conv_last_layer=dilated_conv_last_layer)

# %% ../../nbs/networks/02_res_se_net.ipynb 74
def resnet18(ic, c1_ks, c1_stride, base_pool=True, norm=nn.BatchNorm3d, act=act_gr):
    return ResNet(ic=ic, ip=[64, 128, 256, 512], layers=[2, 2, 2, 2], block_type="basic", \
                 c1_ks=c1_ks, c1_stride=c1_stride, norm=norm, act=act, base_pool=base_pool)

# %% ../../nbs/networks/02_res_se_net.ipynb 77
def resnet50(ic, c1_ks, c1_stride, base_pool=True, norm=nn.BatchNorm3d, act=act_gr):
    return ResNet(ic=ic, ip=[64, 128, 256, 512], layers=[3, 4, 6, 3], block_type="bottleneck", \
                 c1_ks=c1_ks, c1_stride=c1_stride, norm=norm, act=act, base_pool=base_pool)
