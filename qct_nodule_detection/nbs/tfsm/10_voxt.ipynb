{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c560e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tfsm/voxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d61fcd",
   "metadata": {},
   "source": [
    "we will look into transforms like \n",
    "- RandomCrop keeping foreground objects \n",
    "- padding \n",
    "- randomflip \n",
    "- random erase \n",
    "- cutmix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43eea846",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fde1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import numpy as np\n",
    "import fastcore.all as fc\n",
    "from typing import Union, Tuple, List\n",
    "from voxdet.tfsm.standard import BaseT\n",
    "from voxdet.tfsm.utils import corner_2_chwd, chwd_2_corner\n",
    "from voxdet.bbox_func.bbox_iou import calculate_iou\n",
    "import math\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d22f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from voxdet.utils import vis, image_grid, thumbnail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c8613",
   "metadata": {},
   "source": [
    "## Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111338ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Norm3d(BaseT):\n",
    "    def __init__(self, a_min:float, a_max:float, b_min:float=None, b_max:float=None, clip:bool=True, renorm:bool=False): \n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "    __repr__ = fc.basic_repr(flds=\"a_min, a_max, b_min, b_max, clip\")\n",
    "    \n",
    "    def apply(self, img:dict):\n",
    "        assert \"images\" in img.keys(), \"images key is not present\"\n",
    "        fimg = img[\"images\"]\n",
    "        nimg = {}\n",
    "        nimg[\"images\"] = self.apply_image(fimg)\n",
    "        for i in img.keys(): \n",
    "            if i not in nimg.keys(): nimg[i] = img[i]\n",
    "        return nimg \n",
    "    \n",
    "    def apply_image(self, img):\n",
    "        if self.a_max - self.a_min == 0.0: \n",
    "            logger.warning(\"Divide by zero (a_min == a_max)\")\n",
    "            if self.b_min is None: return img - self.a_min\n",
    "            return img - self.a_min + self.b_min \n",
    "        img = (img - self.a_min) / (self.a_max - self.a_min)\n",
    "        if (self.b_min is not None) and (self.b_max is not None):\n",
    "            img = img * (self.b_max - self.b_min) + self.b_min\n",
    "        if self.clip:\n",
    "            img = np.clip(img, self.b_min, self.b_max)\n",
    "        if self.renorm:\n",
    "            img = 2*img - 1 # renormalize the image from [-1, 1]\n",
    "        return img.astype(np.float32)\n",
    "    \n",
    "    def reverse_apply(self, img: dict): return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d486daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = np.ones((10, 10, 10))*2018\n",
    "norm = Norm3d(a_min = -1024.0,\n",
    "      a_max = 300.0,\n",
    "      b_min = 0.0,\n",
    "      b_max = 1.0,\n",
    "      clip = True)\n",
    "out = norm({\"images\": scan})\n",
    "assert out[\"images\"].max() == 1, \"something is wrong with Norm3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6014745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = np.random.beta(5, 2, (10, 10, 10))\n",
    "norm = Norm3d(a_min = -1024.0,\n",
    "      a_max = 300.0,\n",
    "      b_min = 0.0,\n",
    "      b_max = 1.0,\n",
    "      clip = True,\n",
    "      renorm = True)\n",
    "out = norm({\"images\": scan})\n",
    "assert out[\"images\"].max() <= 1, \"something is wrong with Norm3d\"\n",
    "assert out[\"images\"].min() >= -1, \"something is wrong with Norm3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ab09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan4d = np.ones((2, 10, 10, 10))*2018\n",
    "norm = Norm3d(a_min = -1024.0,\n",
    "      a_max = 300.0,\n",
    "      b_min = 0.0,\n",
    "      b_max = 1.0,\n",
    "      clip = True)\n",
    "out4d = norm({\"images\": scan4d})\n",
    "assert out4d[\"images\"].max() == 1, \"something is wrong with Norm3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133dd91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"images\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185ebdc",
   "metadata": {},
   "source": [
    "## Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3b78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((96+32, 192+96, 192+96)) #zyx\n",
    "bbox = np.asarray([[10, 10, 10, 20, 20, 20], [80, 80, 80, 120, 120, 90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28195822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((148, 308, 308), (128, 288, 288))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = 10\n",
    "pad = (pad,)*3 if isinstance(pad, int) else pad\n",
    "pimg = np.zeros(np.asarray(img.shape) + 2*np.asarray(pad))\n",
    "pimg[pad[0]:-pad[0], pad[1]:-pad[1], pad[2]:-pad[2]] = img\n",
    "pimg.shape, img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8c264e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20,  20,  20,  30,  30,  30],\n",
       "       [ 90,  90,  90, 130, 130, 100]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox + np.asarray(pad+pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff476f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def pad3d(img: np.ndarray, bbox: np.ndarray, pad:Union[int, Tuple[int]], side=\"both\"):\n",
    "    \"\"\"img is [zyxzyx], bbox[zyx] pad[zyx]\n",
    "    Pads the image in all direction. we usually gets twice the size of the asked padding.\n",
    "    side takes \"left\" \"right\" \"both\" values\n",
    "    \"\"\"\n",
    "    if side not in [\"left\", \"right\", \"both\"]: raise NotImplementedError(f\"side cannot be {side}\")\n",
    "    \n",
    "    pad = (pad,)*3 if isinstance(pad, int) else pad\n",
    "    n = 2 if side == \"both\" else 1\n",
    "    \n",
    "    multi_view = len(img.shape) == 4\n",
    "    img_shape = img.shape[1:] if multi_view else img.shape\n",
    "    pimg = np.zeros(np.asarray(img_shape) + n*np.asarray(pad))\n",
    "    if multi_view:\n",
    "        pimg = np.expand_dims(pimg, axis=0)\n",
    "        pimg = np.repeat(pimg, img.shape[0], axis=0)\n",
    "    \n",
    "    if side == \"right\": pad = (0, 0, 0)\n",
    "    ims = np.asarray(img_shape) + np.asarray(pad)\n",
    "    if multi_view:\n",
    "        pimg[:, pad[0]:ims[0], pad[1]:ims[1], pad[2]:ims[2]] = img\n",
    "    else:\n",
    "        pimg[pad[0]:ims[0], pad[1]:ims[1], pad[2]:ims[2]] = img\n",
    "    \n",
    "    if bbox is None: return pimg \n",
    "    pbbox = bbox + np.asarray(tuple(pad)+tuple(pad))\n",
    "    return pimg, pbbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f3ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.ones((10, 10, 10))\n",
    "bbox = np.asarray([[1, 1, 2, 2, 4, 4], [3, 3, 4, 4, 5, 5]])\n",
    "pad = 2 \n",
    "pimg, pbbox = pad3d(img, bbox, pad)\n",
    "fc.all_equal(pimg.shape, (14, 14, 14))\n",
    "fc.all_equal(bbox+2, pbbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29463f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = np.ones((2, 10, 10, 10))\n",
    "bbox = np.asarray([[1, 1, 2, 2, 4, 4], [3, 3, 4, 4, 5, 5]])\n",
    "pad = 2 \n",
    "pimg2, pbbox = pad3d(img2, bbox, pad)\n",
    "fc.all_equal(pimg2.shape, (2, 14, 14, 14))\n",
    "fc.all_equal(bbox+2, pbbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4cd2eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAUCAIAAABAqPnNAAAAM0lEQVR4nO3OMQEAMAjEwFL/nh8DLJlgyCm4SvLu+duBmS3CFmGLsEXYImwRtghbhC2iATR6AyWWver0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=50x20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid([Image.fromarray(i*255) for i in img], 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e65c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGIAAAAcCAIAAAAV7/3oAAAAYUlEQVR4nO3XMQqAMBAAQeP//xwbsZQVVBRm6iMH24RbFrjLeO7pOefZ4jF+NLmeTHCQKZEpkSmRKZEpkSmRKZEJ4Lf2Q/n7N/rVyXv56RKZEpkSmRKZEpkSmRKZEpngZRvU/x4nWB3w3wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=98x28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid([Image.fromarray(i*255) for i in pimg], 2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f414bf",
   "metadata": {},
   "source": [
    "### Pad left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e49596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pimg, pbbox = pad3d(img, bbox, pad, side=\"left\")\n",
    "fc.all_equal(pimg.shape, (12, 12, 12))\n",
    "fc.all_equal(bbox+2, pbbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b90652fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEgAAAAYCAIAAADWASznAAAASUlEQVR4nO3WMQoAIAzF0Nb737kuznVRMZC3Kkq2HyGWPPtcVXWfZT67M5ozNMNoDKMxjMYw6a61Ff/ZeG7FDcNoDKMxjMYwmgl0sB4hgjCxEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=72x24>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid([Image.fromarray(i*255) for i in pimg], 2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62fd84",
   "metadata": {},
   "source": [
    "### Pad right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "812c5d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pimg, pbbox = pad3d(img, bbox, pad, side=\"right\")\n",
    "fc.all_equal(pimg.shape, (12, 12, 12))\n",
    "fc.all_equal(bbox, pbbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d71741ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEgAAAAYCAIAAADWASznAAAARUlEQVR4nO3WsQ0AIAzEwDz77xwmgAokjHz1Ny4iJd1da0mqirgZmwWaYTSG0RhGY5h0V1778U759sYMozGMxjAaw6S7JgA7Hie80qEbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=72x24>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid([Image.fromarray(i*255) for i in pimg], 2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da9f57",
   "metadata": {},
   "source": [
    "### Pad using Rsize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "072ebc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.ones((10, 10, 10))\n",
    "rsize = (13, 14, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3d63c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0]), array([1., 2., 1.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_shape = np.maximum(np.asarray(rsize) - np.asarray(img.shape), 0)\n",
    "both_pad = np.floor(pad_shape/2)\n",
    "right_pad = pad_shape%2\n",
    "right_pad, both_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b74655f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.all_equal(right_pad+2*both_pad+img.shape, rsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5238df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pad3d_by_rsize(img: np.ndarray, bbox: np.ndarray, rsize: Tuple[int], side=\"symmetric\"):\n",
    "    \"\"\"img is [zyxzyx], bbox[zyx] crop[zyx]\"\"\"\n",
    "    img_shape = img.shape[1:] if len(img.shape) == 4 else img.shape\n",
    "    pad_shape = np.maximum(np.asarray(rsize) - np.asarray(img_shape), 0)\n",
    "    if side != \"symmetric\": return pad3d(img, bbox, pad_shape.astype(int), side=side)\n",
    "    both_pad = np.floor(pad_shape/2).astype(int)\n",
    "    right_pad = pad_shape%2\n",
    "    if bbox is None:\n",
    "        bimg = pad3d(img, None, both_pad, side=\"both\")\n",
    "        rimg = pad3d(bimg, None, right_pad, side=\"right\")\n",
    "        return rimg\n",
    "    bimg, bbbox = pad3d(img, bbox, both_pad, side=\"both\")\n",
    "    rimg, rbbox = pad3d(bimg, bbbox, right_pad, side=\"right\")\n",
    "    return rimg, rbbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df82779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.ones((10, 10, 10))\n",
    "rsize = (15, 14, 12)\n",
    "rimg = pad3d_by_rsize(img, None, rsize)\n",
    "fc.all_equal(rimg.shape, rsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf58b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = np.ones((2, 10, 10, 10))\n",
    "rsize = (15, 14, 12)\n",
    "rimg2 = pad3d_by_rsize(img2, None, rsize)\n",
    "fc.all_equal(rimg2.shape[1:], rsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c03cd125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADwAAAAqCAIAAABgEa+jAAAAWElEQVR4nO3WMQ4AIAjAQPH/f8YfOBBSg+nNRDsR1hIjGt/KzNtPEV0zu5T3mNEUoylGU4yWVNN2eZEzI7eH0RSjKUZTjJZU03mdYUZuD6MpRlOMphit3xy8yx46c1wmxQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=60x42>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid([Image.fromarray(i*255) for i in rimg], 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa196155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADwAAAAqCAIAAABgEa+jAAAASElEQVR4nO3WsQ3AQAwDMTv77+yf4FOkEByArFVcqZ6Zuuvuqtq2eV4Wa4lOEZ0iOkU08E1ve3Be3iaiU0SniE75ZTQAAMB+ByhzHieq6Y/CAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=60x42>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rimg = pad3d_by_rsize(img, None, rsize, side=\"right\")\n",
    "image_grid([Image.fromarray(i*255) for i in rimg], 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dafe44ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.all_equal(pad3d_by_rsize(img, None, (21, 42, 12)).shape, (21, 42, 12))\n",
    "fc.all_equal(pad3d_by_rsize(img, None, (8, 7, 6)).shape, (10, 10, 10))\n",
    "fc.all_equal(pad3d_by_rsize(img, None, (12, 12, 6)).shape, (12, 12, 10))\n",
    "fc.all_equal(pad3d_by_rsize(img, None, (12, 6, 6)).shape, (12, 10, 10))\n",
    "fc.all_equal(pad3d_by_rsize(img, None, (14, 18, 17)).shape, (14, 18, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb11028e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.all_equal(pad3d_by_rsize(img2, None, (21, 42, 12)).shape[1:], (21, 42, 12))\n",
    "fc.all_equal(pad3d_by_rsize(img2, None, (8, 7, 6)).shape[1:], (10, 10, 10))\n",
    "fc.all_equal(pad3d_by_rsize(img2, None, (12, 12, 6)).shape[1:], (12, 12, 10))\n",
    "fc.all_equal(pad3d_by_rsize(img2, None, (12, 6, 6)).shape[1:], (12, 10, 10))\n",
    "fc.all_equal(pad3d_by_rsize(img2, None, (14, 18, 17)).shape[1:], (14, 18, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef0c4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bbox(rsize) -> None:\n",
    "    bbox = np.asarray([[1, 2, 3, 4, 5, 6]])\n",
    "    b = bbox[0]\n",
    "    img = np.ones((10, 10, 10))\n",
    "    img2 = np.ones((2, 10, 10, 10))\n",
    "    \n",
    "    img[b[0]: b[3], b[1]:b[4], b[2]:b[5]] = 5\n",
    "    img2[:, b[0]: b[3], b[1]:b[4], b[2]:b[5]] = 5\n",
    "\n",
    "    pimg, pbbox = pad3d_by_rsize(img, bbox, rsize)\n",
    "    pimg2, pbbox = pad3d_by_rsize(img2, bbox, rsize)\n",
    "\n",
    "    pb = pbbox[0]\n",
    "\n",
    "    fc.all_equal(img[b[0]: b[3], b[1]:b[4], b[2]:b[5]], pimg[pb[0]: pb[3], pb[1]:pb[4], pb[2]:pb[5]])\n",
    "    fc.all_equal(img2[:, b[0]: b[3], b[1]:b[4], b[2]:b[5]], pimg2[:, pb[0]: pb[3], pb[1]:pb[4], pb[2]:pb[5]])\n",
    "\n",
    "test_bbox((21, 42, 12))\n",
    "test_bbox((8, 7, 6))\n",
    "test_bbox((12, 12, 6))\n",
    "test_bbox((12, 6, 6))\n",
    "test_bbox((14, 18, 17))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fbea3",
   "metadata": {},
   "source": [
    "### Pad by a divisible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d717949c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((85, 65, 58), array([512., 512., 512.]), array([427., 447., 454.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = 512\n",
    "img_shape = (np.random.randint(96), np.random.randint(192), np.random.randint(192))\n",
    "new_shape = np.ceil(np.asarray(img_shape)/sd)*sd\n",
    "pad_size = np.maximum(np.asarray(new_shape) - np.asarray(img_shape), 0)\n",
    "img_shape, new_shape, pad_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eb39804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def pad3d_by_div(img: np.ndarray, bbox: np.ndarray, size_divisible: int=4, side: str= \"symmetric\"):\n",
    "    \"\"\"img is [zyxzyx], bbox[zyx] set the final img dim should be divisible by {size_divisible}\"\"\"\n",
    "    img_shape = img.shape[1:] if len(img.shape) == 4 else img.shape\n",
    "    new_shape = (np.ceil(np.asarray(img_shape)/size_divisible)*size_divisible).astype(int)\n",
    "    return pad3d_by_rsize(img, bbox, new_shape, side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "202fc3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32, 32), array([[11, 11, 11, 11, 11, 11]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.ones((11, 11, 11))\n",
    "bbox = np.asarray([[1, 1, 1, 1, 1, 1]])\n",
    "img, bbox = pad3d_by_div(img, bbox, size_divisible=32)\n",
    "img.shape, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b97fad91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 32, 32, 32), array([[11, 11, 11, 11, 11, 11]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img2 = np.ones((2, 11, 11, 11))\n",
    "bbox = np.asarray([[1, 1, 1, 1, 1, 1]])\n",
    "img2, bbox = pad3d_by_div(img2, bbox, size_divisible=32)\n",
    "img2.shape, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0295ab2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAACACAIAAABr1yBdAAAAxklEQVR4nO3cQQqEMBAAQeP//xx/oLsEUemqc5iEQF9n2wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD4z3j6Afeac54fGGPpB8z/+vx95Xr4OgGQJgDSBECaAEgTAGkCIE0ApAmANAEAAAAAAAAAAAAAAAAAAAAAvNHF9vT373c3f2U+1qKQJgDSBECaAEgTAGkCIE0ApAmANAGQJgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPjNAXs/ISuACtSBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x128>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid([Image.fromarray(i*255) for i in img], 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc2173",
   "metadata": {},
   "source": [
    "## PadIfNeeded Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a596542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class PadIfNeeded(BaseT):\n",
    "    def __init__(self, sd=None, img_size=None, side: str=\"right\"):\n",
    "        \"\"\"sd is size divisible, img_size in zyx format. One of it should be None\"\"\"\n",
    "        fc.store_attr()\n",
    "        super().__init__()\n",
    "        assert self.side in [\"right\"], \"only right side is implemented\"\n",
    "        if (self.sd is None) and (self.img_size is None): raise ValueError(\"Both intputs sd and img_size cant be None\")\n",
    "        if None not in (self.sd, self.img_size): raise ValueError(\"One of sd or img_size should be None. Both are given\")\n",
    "    \n",
    "    __repr__ = fc.basic_repr(\"sd, img_size, side\")\n",
    "    \n",
    "    def apply(self, img: dict):\n",
    "        assert \"images\" in img.keys(), f\"images not present in input [img]. Only: {img.keys()} present\"\n",
    "        fimg = img[\"images\"].copy()\n",
    "        bbox = img[\"boxes\"].copy() if \"boxes\" in img.keys() else None\n",
    "        nimg = {}\n",
    "        out = pad3d_by_div(fimg, bbox, self.sd, self.side) if self.sd is not None else pad3d_by_rsize(fimg, bbox, self.img_size, self.side)\n",
    "        if bbox is None: nimg[\"images\"] = out\n",
    "        else: nimg[\"images\"], nimg[\"boxes\"] = out\n",
    "        for i in img.keys():\n",
    "            if i not in nimg.keys(): nimg[i] = img[i]\n",
    "        return nimg\n",
    "    \n",
    "    def reverse_apply(self, img: dict): \n",
    "        if self.side != \"right\": raise ValueError(\"only right allowed\")\n",
    "        return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24e36ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.test_fail(PadIfNeeded, kwargs=dict(sd=None, img_size=None))\n",
    "fc.test_fail(PadIfNeeded, kwargs=dict(sd=32, img_size=(100, 100)))\n",
    "fc.test_fail(PadIfNeeded, kwargs=dict(sd=32, side=\"asymetric\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ee0cded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.PadIfNeeded(sd=32, img_size=None, side='right')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pin = PadIfNeeded(sd=32)\n",
    "pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9469e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = {}\n",
    "img[\"images\"] = np.ones((12, 13, 14))\n",
    "img[\"boxes\"] = np.empty((0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2486d735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32, 32),\n",
       " (12, 13, 14),\n",
       " array([], shape=(0, 6), dtype=float64),\n",
       " array([], shape=(0, 6), dtype=float64))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nimg = pin(img)\n",
    "nimg[\"images\"].shape, img[\"images\"].shape, nimg[\"boxes\"], img[\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93e3c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = {}\n",
    "img[\"images\"] = np.ones((12, 13, 14))\n",
    "img[\"boxes\"] = np.asarray([[2, 6, 3, 1, 4, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df483628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32, 32),\n",
       " (12, 13, 14),\n",
       " array([[2, 6, 3, 1, 4, 8]]),\n",
       " array([[2, 6, 3, 1, 4, 8]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nimg = pin(img)\n",
    "nimg[\"images\"].shape, img[\"images\"].shape, nimg[\"boxes\"], img[\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4252bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = {}\n",
    "img2[\"images\"] = np.ones((2, 42, 33, 14))\n",
    "img2[\"boxes\"] = np.asarray([[2, 6, 3, 1, 4, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "652f1b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 64, 64, 32),\n",
       " (2, 42, 33, 14),\n",
       " array([[2, 6, 3, 1, 4, 8]]),\n",
       " array([[2, 6, 3, 1, 4, 8]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nimg2 = pin(img2)\n",
    "nimg2[\"images\"].shape, img2[\"images\"].shape, nimg2[\"boxes\"], img2[\"boxes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669d579",
   "metadata": {},
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b09a16",
   "metadata": {},
   "source": [
    "```\n",
    "xyz -> (1, 2, 0) -> yzx -> (2, 0, 1) -> xyz\n",
    "xyz -> (1, 0, 2) -> yxz -> (1, 0, 2) -> xyz\n",
    "xyz -> (2, 1, 0) -> zyx -> (2, 1, 0) -> xyz\n",
    "xyz -> (2, 0, 1) -> zxy -> (1, 2, 0) -> xyz\n",
    "xyz -> (0, 2, 1) -> xzy -> (0, 2, 1) -> xyz\n",
    "xyz -> (0, 1, 2) -> xyz -> (0, 1, 2) -> xyz\n",
    "```\n",
    "xyz -> (1, 2, 0) -> yzx -> (2, 0, 1) -> xyz\n",
    "xyz -> (2, 0, 1) -> zxy -> (1, 2, 0) -> xyz\n",
    "\n",
    "xyz -> (1, 0, 2) -> yxz -> (1, 0, 2) -> xyz\n",
    "xyz -> (2, 1, 0) -> zyx -> (2, 1, 0) -> xyz\n",
    "\n",
    "\n",
    "```\n",
    "xyzxyz -> (1, 2, 0, 4, 5, 3) -> yzxyzx -> (2, 0, 1, 5, 3, 4) -> xyz\n",
    "xyz -> (1, 0, 2) -> yxz -> (1, 0, 2) -> xyz\n",
    "xyz -> (2, 1, 0) -> zyx -> (2, 1, 0) -> xyz\n",
    "xyz -> (2, 0, 1) -> zxy -> (1, 2, 0) -> xyz\n",
    "xyz -> (0, 2, 1) -> xzy -> (0, 2, 1) -> xyz\n",
    "xyz -> (0, 1, 2) -> xyz -> (0, 1, 2) -> xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eeb506d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transpose(BaseT):\n",
    "    def __init__(self, order: tuple= None):\n",
    "        fc.store_attr()\n",
    "        super().__init__()\n",
    "        assert len(order) ==3, f\"only order 3 is allowed for now. given: {self.order}\"\n",
    "        self.rorder = self.reverse_order()\n",
    "        self.boxo = self.order + tuple([i+len(self.order) for i in self.order])\n",
    "        self.boxro = self.rorder + tuple([i+len(self.rorder) for i in self.rorder])\n",
    "    \n",
    "    __repr__ = fc.basic_repr(flds=\"order\")\n",
    "    \n",
    "    def apply(self,  img):\n",
    "        if self.order is None: return img\n",
    "        assert \"images\" in img.keys(), \"images key not Present\"\n",
    "        fimg = img[\"images\"].copy()\n",
    "        assert len(self.order) == len(fimg.shape), f\"order: {self.order} and img : {fimg.shape} is not same\"\n",
    "        nimg={}\n",
    "        nimg[\"images\"] = self.apply_img(fimg, self.order)\n",
    "        if \"boxes\" in img.keys(): nimg[\"boxes\"] = self.apply_bbox(img[\"boxes\"], self.boxo)\n",
    "        if \"spacing\" in img.keys(): nimg[\"spacing\"] = [img['spacing'][i] for i in self.order]\n",
    "        for i in img.keys():\n",
    "            if i not in nimg.keys(): nimg[i] = img[i]\n",
    "        return nimg\n",
    "    \n",
    "    def apply_img(self, img, order): return img.transpose(order)\n",
    "\n",
    "    def apply_bbox(self, bbox, order):\n",
    "        out = bbox.copy()\n",
    "        out = out[:, order]\n",
    "        return out \n",
    "    \n",
    "    def reverse_apply(self, img: dict):\n",
    "        assert \"images\" in img.keys()\n",
    "        nimg = {}\n",
    "        nimg[\"images\"] = self.apply_img(img[\"images\"], self.rorder)\n",
    "        if \"boxes\" in img.keys(): nimg[\"boxes\"] = self.apply_bbox(img[\"boxes\"], self.boxro)\n",
    "        if \"spacing\" in img.keys(): nimg[\"spacing\"] = [img['spacing'][i] for i in self.rorder]\n",
    "        for i in img.keys():\n",
    "            if i not in nimg.keys(): nimg[i] = img[i]\n",
    "        return nimg\n",
    "    \n",
    "    def reverse_order(self):\n",
    "        o = \"\".join(map(lambda x: str(x), self.order))\n",
    "        mapper = {\"120\": \"201\", \"102\": \"102\", \"210\": \"210\", \"201\": \"120\", \"021\": \"021\", \"012\": \"012\"}\n",
    "        return tuple([int(i) for i in mapper[o]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6888b611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Transpose(order=(1, 2, 0))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = Transpose(order=(1, 2, 0))\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b66d7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] (12, 13, 14) [] (13, 14, 12)\n"
     ]
    }
   ],
   "source": [
    "img = {}\n",
    "img[\"images\"] = np.ones((12, 13, 14))\n",
    "img[\"boxes\"] = np.empty((0, 6))\n",
    "trimg = tr(img)\n",
    "print(img[\"boxes\"], img[\"images\"].shape, trimg[\"boxes\"], trimg[\"images\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b26bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2, 0), (2, 0, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.order, tr.rorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b643c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] (12, 13, 14) [] (13, 14, 12)\n"
     ]
    }
   ],
   "source": [
    "trimg = tr(img)\n",
    "print(img[\"boxes\"], img[\"images\"].shape, trimg[\"boxes\"], trimg[\"images\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "221cbf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 13, 14), array([], shape=(0, 6), dtype=float64))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = tr.reverse_apply(trimg)\n",
    "rr[\"images\"].shape, rr[\"boxes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaaf66",
   "metadata": {},
   "source": [
    "## RandomCrop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "831d75fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16096.0,\n",
       " (331, 400, 400),\n",
       " array([[330, 238,  83, 331, 250,  91],\n",
       "        [ 80,  80,  80,  90, 120, 120]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.zeros((331, 400, 400)) #zyx\n",
    "#bbox = np.asarray([[ 91   , 238 ,  83, 100   , 250 , 91], [80, 80, 80, 90, 120, 120]]) #xyz\n",
    "bbox = np.asarray([[330, 238, 83, 331, 250, 91], [80, 80, 80, 90, 120, 120]])\n",
    "bbox2 = np.empty((0, 6))\n",
    "for b in bbox:\n",
    "    img[int(b[0]): int(b[3]), int(b[1]):int(b[4]), int(b[2]): int(b[5])] = 1\n",
    "img.sum(), img.shape, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6e89c",
   "metadata": {},
   "source": [
    "### steps and issues involved\n",
    "- No bbox \n",
    "- image_size is smaller than what is required. \n",
    "\n",
    "\n",
    "### Algo:\n",
    "- select a crop size (96, 192, 192) in this case\n",
    "- select a box and find its center: create a grid \n",
    "- select one point as center and crop the image from that space. \n",
    "- realign bboxes to this new crop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8c67d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 80,  80,  80,  90, 120, 120]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_size = (96, 192, 192)\n",
    "b1 = bbox[np.random.randint(len(bbox))].reshape(1, -1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53c3612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5., 20., 20.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b1[:, 3:] - b1[:, :3])/2\n",
    "# out[:, 3:] = hwd\n",
    "# out[:, :3] = bbox[:, :3]+hwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81ec28e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 85., 100., 100.,  10.,  40.,  40.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1c = corner_2_chwd(b1)[0]\n",
    "b1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510ebe5",
   "metadata": {},
   "source": [
    "> Get all the centers keeping bbox center as centr and crop_size as size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6e32cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([37.,  4.,  4.]),\n",
       " array([133., 196., 196.]),\n",
       " array([ 85., 100., 100.,  10.,  40.,  40.]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = b1c[:3]- np.asarray(crop_size)/2\n",
    "y = b1c[:3]+ np.asarray(crop_size)/2\n",
    "x, y, b1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a66f4acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([37.,  4.,  4.]), array([133., 196., 196.]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all centers which are outside image \n",
    "x = np.maximum(x, 0)\n",
    "y = np.minimum(y, img.shape)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adea903f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([48., 96., 96.]), array([133., 196., 196.]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to select boxes which are inside image \n",
    "x = np.maximum(x, np.asarray(crop_size)/2)#.astype(int)\n",
    "y = np.minimum(y, np.asarray(img.shape) - np.asarray(crop_size)/2)#.astype(int)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d1a1a",
   "metadata": {},
   "source": [
    "previously we did this but the problem is that when x is 282.5 , we floor it 282 and this takes the bounding box away from image.\n",
    "```\n",
    "x = np.floor(x).astype(int)\n",
    "y = np.ceil(y).astype(int)\n",
    "x, y\n",
    "```\n",
    "so we reversed this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb5c8bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([48, 96, 96]), array([133, 196, 196]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ceil(x).astype(int)\n",
    "y = np.floor(y).astype(int)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2dcaa5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877286, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrs = np.mgrid[x[0]:y[0]+1:1, x[1]:y[1]+1:1, x[2]:y[2]+1:1].reshape(3, -1).T\n",
    "ctrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "251ba574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 48,  96,  96],\n",
       "       [ 48,  96,  97],\n",
       "       [ 48,  96,  98],\n",
       "       ...,\n",
       "       [133, 196, 194],\n",
       "       [133, 196, 195],\n",
       "       [133, 196, 196]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a885f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,  96., 192., 192.],\n",
       "       [  0.,   0.,   1.,  96., 192., 193.],\n",
       "       [  0.,   0.,   2.,  96., 192., 194.],\n",
       "       ...,\n",
       "       [ 85., 100.,  98., 181., 292., 290.],\n",
       "       [ 85., 100.,  99., 181., 292., 291.],\n",
       "       [ 85., 100., 100., 181., 292., 292.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrs2 = np.hstack([ctrs - np.asarray(crop_size)/2, ctrs + np.asarray(crop_size)/2])\n",
    "ctrs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2ec12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voxdet.bbox_func.bbox_iou import calculate_iou_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36fbdfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 80,  80,  80,  90, 120, 120]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca44ede8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877286, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = calculate_iou_numpy(ctrs2, b1)\n",
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9753952c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00452112, 0.00452112, 0.00452112, ..., 0.00061924, 0.00059108,\n",
       "       0.00056291])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[tt>0] #we have to select ctrs which have iou with the bounding box >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54784d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877286, 6)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrs2[tt.reshape(-1)>0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3948d",
   "metadata": {},
   "source": [
    "> For self-sup there won't be any gt box to pick up. In that case it is better to select a random point and from that generate bbox.\n",
    "- That bbox can be anywhere within the image.\n",
    "\n",
    "\n",
    "we will not be using the below for time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c71e8108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135,  76, 355, 145,  86, 365]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(img.shape)\n",
    "x = np.concatenate([x-np.asarray([5, 5, 5]), x+np.asarray([5, 5, 5])])\n",
    "x[:3] = np.maximum(x[:3], np.asarray([0, 0, 0]))\n",
    "x[3:] = np.minimum(x[3:], img.shape)\n",
    "x.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92418983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_bbox(img_shape, margin=(5, 5, 5)):\n",
    "    x = np.random.randint(img_shape)\n",
    "    x = np.concatenate([x-np.asarray(margin), x+np.asarray(margin)])\n",
    "    x[:3] = np.maximum(x[:3], np.asarray([0, 0, 0]))\n",
    "    x[3:] = np.minimum(x[3:], img_shape)\n",
    "    return x.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c9572b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[318, 273, 314, 328, 283, 324]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random_bbox(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac510df9",
   "metadata": {},
   "source": [
    "> Combine Everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84c3ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_valid_centers(base: np.ndarray, crop_size: Tuple[int], img_shape: Tuple[int]):\n",
    "    \"\"\"base: [z, y, z, z, y, x] crop_size: (z, y, x), img_size: (z, y, x)\"\"\"\n",
    "    b1c = corner_2_chwd(base)[0]\n",
    "    x = b1c[:3]- np.asarray(crop_size)/2\n",
    "    y = b1c[:3]+ np.asarray(crop_size)/2\n",
    "    # Remove all centers which are outside image \n",
    "    x = np.maximum(x, 0)\n",
    "    y = np.minimum(y, img_shape)\n",
    "    # we need to select boxes which are inside image \n",
    "    x = np.maximum(x, np.asarray(crop_size)/2)\n",
    "    y = np.minimum(y, np.asarray(img_shape) - np.asarray(crop_size)/2)\n",
    "    #cast to nearest int\n",
    "    #x = np.floor(x).astype(int)\n",
    "    #y = np.ceil(y).astype(int)\n",
    "    x = np.ceil(x).astype(int)\n",
    "    y = np.floor(y).astype(int)\n",
    "    ctrs = np.mgrid[x[0]:y[0]+1:1, x[1]:y[1]+1:1, x[2]:y[2]+1:1].reshape(3, -1).T\n",
    "    \n",
    "    #calculate all the crops\n",
    "    #ctrs2 = np.hstack([ctrs - np.asarray(crop_size)/2, ctrs + np.asarray(crop_size)/2])\n",
    "    #now make sure that select base is which in the crop selected \n",
    "    #tt = calculate_iou_numpy(ctrs2, base)\n",
    "    #ctrs = ctrs[tt.reshape(-1)>0]\n",
    "    return ctrs , x ,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "617b05f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 48,  96,  96],\n",
       "       [ 48,  96,  97],\n",
       "       [ 48,  96,  98],\n",
       "       ...,\n",
       "       [133, 196, 194],\n",
       "       [133, 196, 195],\n",
       "       [133, 196, 196]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea3b30f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.94 ms, sys: 1.58 ms, total: 6.53 ms\n",
      "Wall time: 5.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 48,  96,  96],\n",
       "        [ 48,  96,  97],\n",
       "        [ 48,  96,  98],\n",
       "        ...,\n",
       "        [133, 196, 194],\n",
       "        [133, 196, 195],\n",
       "        [133, 196, 196]]),\n",
       " (877286, 3))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time ctrs ,x ,y= get_valid_centers(b1, crop_size, img.shape)\n",
    "ctrs, ctrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "051ac28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time ctrs = get_valid_centers(None, crop_size, img.shape)\n",
    "# ctrs, ctrs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a308007d",
   "metadata": {},
   "source": [
    "> Randomly select one point as center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3fe7eb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([129, 196, 148])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_center = ctrs[np.random.randint(len(ctrs))]\n",
    "crop_center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c25c7e9",
   "metadata": {},
   "source": [
    ">crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c995387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81., 100.,  52., 177., 292., 244.])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = crop_center - np.asarray(crop_size)/2\n",
    "y = crop_center + np.asarray(crop_size)/2\n",
    "nbox = np.hstack([x, y])\n",
    "nbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2441e522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 192, 192), 7200.0)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1, y1, x1, z2, y2, x2 = nbox.astype(np.int32) \n",
    "nimg = img[z1: z2, y1:y2, x1:x2]\n",
    "nimg.shape, nimg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d07fbbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[330, 238,  83, 331, 250,  91],\n",
       "       [ 80,  80,  80,  90, 120, 120]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d819232a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81, 100,  52, 177, 292, 244], dtype=int32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbox.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9185f",
   "metadata": {},
   "source": [
    "> bbox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "01fdca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def volume(box):\n",
    "    \"\"\"\n",
    "    Calculate the volume of a box given in [z1, y1, x1, z2, y2, x2] format.\n",
    "    \"\"\"\n",
    "    z1, y1, x1, z2, y2, x2 = box\n",
    "    return abs((z1 - z2) * (y1 - y2) * (x1 - x2))\n",
    "\n",
    "def vol_intersect(bbox, crop_box):\n",
    "    \"\"\"\n",
    "    Calculate the volume of the intersection between two boxes.\n",
    "    Each box is given in [z1, y1, x1, z2, y2, x2] format.\n",
    "    \"\"\"\n",
    "    az1, ay1, ax1, az2, ay2, ax2 = bbox\n",
    "    bz1, by1, bx1, bz2, by2, bx2 = crop_box\n",
    "    \n",
    "    z_overlap = max(0, min(az2, bz2) - max(az1, az1))\n",
    "    y_overlap = max(0, min(ay2, by2) - max(ay1, ay1))\n",
    "    x_overlap = max(0, min(ax2, bx2) - max(ax1, ax1))\n",
    "    \n",
    "    intersection_volume = z_overlap * y_overlap * x_overlap\n",
    "    return intersection_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "141e9453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[330, 238,  83, 331, 250,  91],\n",
       "       [ 80,  80,  80,  90, 120, 120]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1399490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81., 100.,  52., 177., 292., 244.])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b7419e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[330.5, 244. ,  87. ,   1. ,  12. ,   8. ],\n",
       "       [ 85. , 100. , 100. ,  10. ,  40. ,  40. ]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxc = corner_2_chwd(bbox)\n",
    "bboxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "83beafd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[249.5, 144. ,  35. ,   1. ,  12. ,   8. ],\n",
       "       [  4. ,   0. ,  48. ,  10. ,  40. ,  40. ]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxc[:, :3] -= nbox[:3]\n",
    "bboxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2eaaa704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[249., 138.,  31., 250., 150.,  39.],\n",
       "       [ -1., -20.,  28.,   9.,  20.,  68.]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbbox = chwd_2_corner(bboxc)\n",
    "nbbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a47a1",
   "metadata": {},
   "source": [
    "### Remove bboxes outside image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "297d058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_out_of_bounds_bboxes(bboxes, img_shape):\n",
    "    z, y, x = img_shape \n",
    "    zmin, ymin, xmin, zmax, ymax, xmax = bboxes.T\n",
    "    valid_index = np.where((zmin < z) & (ymin < y) & (xmin < x) & (zmax >= 0) & (ymax >= 0) & (xmax >= 0))\n",
    "    return bboxes[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "200b2b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1., -20.,  28.,   9.,  20.,  68.]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbbox2 = remove_out_of_bounds_bboxes(nbbox, crop_size)\n",
    "nbbox2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "473418fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "robb = partial(remove_out_of_bounds_bboxes, img_shape=(100, 100, 100))\n",
    "fc.test_eq(robb(np.asarray([[50, -50, 10, 10, -30, 20]])).shape[0], 0)\n",
    "fc.test_eq(robb(np.asarray([[180, 30, 10, 190, 50, 20]])).shape[0], 0)\n",
    "fc.test_eq(robb(np.asarray([[120, 120, 10, 140, 140, 20]])).shape[0], 0)\n",
    "fc.test_eq(robb(np.asarray([[30, 110, 10, 50, 120, 20]])).shape[0], 0)\n",
    "fc.test_eq(robb(np.asarray([[-20, 30, 10, -5, 50, 20]])).shape[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588a540",
   "metadata": {},
   "source": [
    "## Clip2Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "db213d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 28.,  9., 20., 68.]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbbox2[:, 0] = np.maximum(0, nbbox2[:, 0])  # x1\n",
    "nbbox2[:, 1] = np.maximum(0, nbbox2[:, 1])  # y1\n",
    "nbbox2[:, 2] = np.maximum(0, nbbox2[:, 2])  # y1\n",
    "nbbox2[:, 3] = np.minimum(crop_size[2], nbbox2[:, 3])  # x2\n",
    "nbbox2[:, 4] = np.minimum(crop_size[1], nbbox2[:, 4])  # x2\n",
    "nbbox2[:, 5] = np.minimum(crop_size[0], nbbox2[:, 5])  # y2\n",
    "nbbox2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f296fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def clip_2_img(bbox: np.ndarray, img_size: Tuple[int]):\n",
    "    \"\"\"remove all the boxes which have x2 or y2 or z2 as negative and are outside the img_size completely\"\"\"\n",
    "    bbox = remove_out_of_bounds_bboxes(bbox, img_size)\n",
    "    bbox[:, 0] = np.maximum(0, bbox[:, 0])  # x1\n",
    "    bbox[:, 1] = np.maximum(0, bbox[:, 1])  # y1\n",
    "    bbox[:, 2] = np.maximum(0, bbox[:, 2])  # y1\n",
    "    bbox[:, 3] = np.minimum(img_size[0], bbox[:, 3])  # x2\n",
    "    bbox[:, 4] = np.minimum(img_size[1], bbox[:, 4])  # x2\n",
    "    bbox[:, 5] = np.minimum(img_size[2], bbox[:, 5])  # y2\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5de7ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 28.,  9., 20., 68.]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_2_img(nbbox, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "051d0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def crop_bbox(bbox: np.ndarray, fb: np.ndarray, img_size: Tuple[int], keep_volume: float):\n",
    "    \"\"\"bbox is [zyxzyx] fb is [zyxzyx] and the img_size [zyx]\"\"\"\n",
    "    if fb.shape != (6, ): raise ValueError(f\"fb shape required is (6,). Got {fb.shape}\")\n",
    "    bbox_vol_overlap = np.array([vol_intersect(box, fb)/volume(box) for box in bbox])\n",
    "    keep = bbox_vol_overlap >= keep_volume\n",
    "    new_bbox = bbox[keep]\n",
    "    bboxc = corner_2_chwd(new_bbox.astype(np.float32))\n",
    "    fb = fb.astype(np.float32)\n",
    "    bboxc[:, :3] -= fb[:3]\n",
    "    nbbox = chwd_2_corner(bboxc)\n",
    "    nbbox2 = clip_2_img(nbbox, img_size)\n",
    "    return nbbox2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f6f06d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 28.,  9., 20., 68.]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbbox3 = crop_bbox(bbox, nbox, crop_size,0.5)\n",
    "nbbox3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b8abb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def calculate_occupancy(img_shape, crop_boxes):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of volume occupied by potentially overlapping\n",
    "    smaller cuboids(crop_boxes) within a larger cuboid defined by an img_shape,\n",
    "    \n",
    "    Parameters:\n",
    "    - img_shape: The shape of the image (z, y, x) defining the dimensions of the larger cuboid.\n",
    "    - crop_boxes: coordinates in the format [z1, y1, x1, z2, y2, x2].\n",
    "    \n",
    "    Returns:\n",
    "    - The percentage of the total volume of smaller cuboids relative to the volume of the larger cuboid.\n",
    "    \"\"\"\n",
    "    voxels = np.zeros(img_shape, dtype=bool)\n",
    "    \n",
    "    for row in crop_boxes:\n",
    "        z1, y1, x1, z2, y2, x2 = row\n",
    "        voxels[z1:z2, y1:y2, x1:x2] = True\n",
    "    \n",
    "    occupied_volume = np.sum(voxels)\n",
    "    larger_volume = np.prod(img_shape)\n",
    "    \n",
    "    vol_occupancy = (occupied_volume / larger_volume)\n",
    "    return vol_occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "36996b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def random_crop(img: np.ndarray, bbox: np.ndarray, crop_size: Tuple[int], sample_size: int = 1, keep_volume: float = 0.5):\n",
    "    \"\"\"img: zyx crop_size: zyx bbox: [z, y, x, z, y, x]\"\"\"\n",
    "    img_shape = img.shape if len(img.shape) == 3 else img.shape[1:]\n",
    "    \n",
    "    crops = []\n",
    "    cboxes = []\n",
    "    crop_corners = []\n",
    "    \n",
    "    if len(bbox) >= sample_size :\n",
    "        select_indices = np.random.choice(len(bbox) , size = sample_size ,replace = False)\n",
    "    else : \n",
    "        select_indices = np.random.choice(len(bbox) , size = sample_size)\n",
    "                \n",
    "    for i in select_indices :\n",
    "        b1 = bbox[i][None]\n",
    "        ctrs ,_ ,_ = get_valid_centers(b1, crop_size, img_shape)\n",
    "        crop_center = ctrs[np.random.randint(len(ctrs))]\n",
    "\n",
    "        x = crop_center - np.asarray(crop_size)/2\n",
    "        y = crop_center + np.asarray(crop_size)/2\n",
    "        fb = np.hstack([x, y]).astype(np.int32) #final crop box \n",
    "        \n",
    "        crop_corners.append(fb)\n",
    "\n",
    "        nimg = img[..., fb[0]:fb[3], fb[1]:fb[4], fb[2]:fb[5]]\n",
    "        if (bbox is None) or (bbox.shape[0] == 0):\n",
    "            nbbox = np.empty((0, 6))\n",
    "        else:\n",
    "            nbbox = crop_bbox(bbox, fb, crop_size, keep_volume)\n",
    "        crops.append(nimg)  \n",
    "        cboxes.append(nbbox)\n",
    "        \n",
    "    crop_occupancy = calculate_occupancy(img_shape , crop_corners)\n",
    "    \n",
    "    if len(crops) == 1 :\n",
    "        return crops[0] , cboxes[0] , crop_occupancy\n",
    "        \n",
    "    return crops, cboxes , crop_occupancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f2aadcf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 ms, sys: 3.28 ms, total: 21.3 ms\n",
      "Wall time: 20.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 28.,  9., 20., 68.]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nimg, nbbox, crop_occupancy = random_crop(img, bbox, crop_size)\n",
    "nbbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dfbe3c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.26 s, sys: 428 ms, total: 4.69 s\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_size = 4\n",
    "for i in range(100):\n",
    "    nimg, nbbox, crop_occupancy = random_crop(img, bbox, crop_size, sample_size)\n",
    "    if len(nbbox) < 1 :\n",
    "        print(\"Failed\")\n",
    "    for j in range(sample_size) :\n",
    "        if nimg[j].sum()==0: \n",
    "            print(\"Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "720c290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def select_min_overlap_cuboids(iou_matrix, M):\n",
    "    N = iou_matrix.shape[0]  # Number of cuboids\n",
    "    selected_indices = []  # Indices of selected cuboids\n",
    "    \n",
    "    for _ in range(M):\n",
    "        # Calculate the average IOU of each cuboid with the already selected cuboids\n",
    "        if selected_indices:\n",
    "            avg_iou = np.mean(iou_matrix[selected_indices], axis=0)\n",
    "        else:\n",
    "            # If no cuboids have been selected yet, set initial avg_iou to favor first selection\n",
    "            avg_iou = np.zeros(N)\n",
    "        \n",
    "        # Exclude already selected cuboids by setting their avg_iou to a high value\n",
    "        avg_iou[selected_indices] = np.inf\n",
    "        \n",
    "        # Select the cuboid with the minimal average IOU\n",
    "        next_index = np.argmin(avg_iou)\n",
    "        selected_indices.append(next_index)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1ff72678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.08171213 0.43462516 0.42338548]\n",
      " [0.08171213 0.         0.75041207 0.53626548]\n",
      " [0.43462516 0.75041207 0.         0.66230395]\n",
      " [0.42338548 0.53626548 0.66230395 0.        ]]\n",
      "Selected Cuboid Indices: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "N = 4  # Example: 5 crops\n",
    "M = 2   # Want to select 2 crops\n",
    "iou_matrix = np.random.random((N, N))  # Example IOU matrix, replace with your actual IOU values\n",
    "iou_matrix = (iou_matrix + iou_matrix.T) / 2  # Make the matrix symmetric\n",
    "np.fill_diagonal(iou_matrix, 0)  # Set diagonal to 0, as a cuboid does not intersect with itself\n",
    "\n",
    "print(iou_matrix)\n",
    "selected_indices = select_min_overlap_cuboids(iou_matrix, M)\n",
    "print(\"Selected Cuboid Indices:\", selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b4755e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 1, 0])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 6\n",
    "indices = np.arange(length)\n",
    "# Calculate the distance of each index from the nearest end\n",
    "distances = np.minimum(indices, length - 1 - indices)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7899471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.5, 0.3333333333333333, 0.3333333333333333, 0.5, 1.0]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate probabilities inversely related to the distance (higher near ends)\n",
    "probabilities = 1 /(1 + distances)\n",
    "probabilities.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "39ccba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27272727, 0.13636364, 0.09090909, 0.09090909, 0.13636364,\n",
       "       0.27272727])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize probabilities to sum to 1\n",
    "probabilities /= probabilities.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7d42e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def generate_end_biased_distribution(length):\n",
    "    \"\"\"\n",
    "    Generate a probability distribution with higher probabilities at the ends.\n",
    "    \n",
    "    Parameters:\n",
    "    - length: The total length of the list.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of probabilities.\n",
    "    \"\"\"\n",
    "    # Create an array with indices from 0 to length - 1\n",
    "    indices = np.arange(length)\n",
    "    # Calculate the distance of each index from the nearest end\n",
    "    distances = np.minimum(indices, length - 1 - indices)\n",
    "    # Generate probabilities inversely related to the distance (higher near ends)\n",
    "    probabilities = 1 /(1 + distances)\n",
    "    # Normalize probabilities to sum to 1\n",
    "    probabilities /= probabilities.sum()\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2cfe4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def random_crop_multi(img: np.ndarray, bbox: np.ndarray, crop_size: Tuple[int], sample_size: int, keep_volume: float = 0.5):\n",
    "    \"\"\"img: zyx crop_size: zyx bbox: [z, y, x, z, y, x]\"\"\"\n",
    "    img_shape = img.shape if len(img.shape) == 3 else img.shape[1:]\n",
    "    \n",
    "    sample_per_bbox = math.ceil(sample_size/len(bbox))    \n",
    "    crop_centers = []\n",
    "\n",
    "    for j in range(len(bbox)) :\n",
    "        b1 = bbox[j][None]\n",
    "        _ , x, y = get_valid_centers(b1, crop_size, img_shape)\n",
    "        A = np.arange(x[0],y[0]+1,1)\n",
    "        B = np.arange(x[1],y[1]+1,1)\n",
    "        C = np.arange(x[2],y[2]+1,1)\n",
    "                \n",
    "        if len(A) < sample_per_bbox :\n",
    "            a = np.random.choice(A , sample_per_bbox)\n",
    "        else :\n",
    "            a = np.random.choice(A , sample_per_bbox, replace = False, p = generate_end_biased_distribution(len(A)))\n",
    "        \n",
    "        if len(B) < sample_per_bbox :\n",
    "            b = np.random.choice(B , sample_per_bbox)\n",
    "        else :\n",
    "            b = np.random.choice(B , sample_per_bbox,replace = False,  p = generate_end_biased_distribution(len(B)))\n",
    "            \n",
    "        if len(C) < sample_per_bbox :\n",
    "            c = np.random.choice(C , sample_per_bbox)\n",
    "        else :\n",
    "            c = np.random.choice(C , sample_per_bbox,replace =False,  p = generate_end_biased_distribution(len(C)))\n",
    "            \n",
    "        crop_ctrs = [[a[i] , b[i] , c[i]] for i in range(sample_per_bbox)]\n",
    "        crop_centers.extend(crop_ctrs)\n",
    "                 \n",
    "    crop_corners = []\n",
    "    for crop_center in crop_centers : \n",
    "        x = crop_center - np.asarray(crop_size)/2\n",
    "        y = crop_center + np.asarray(crop_size)/2\n",
    "        fb = np.hstack([x, y]).astype('int').tolist()\n",
    "        crop_corners.append(fb)\n",
    "        \n",
    "    crop_corners = np.array(crop_corners)\n",
    "    \n",
    "        \n",
    "    if len(crop_corners) > sample_size :\n",
    "#         selected_indices = np.random.choice(len(crop_corners) ,size= sample_size ,replace=True)\n",
    "#         crop_corners = crop_corners[selected_indices , :]\n",
    "        overlaps = calculate_iou(crop_corners, crop_corners)\n",
    "        np.fill_diagonal(overlaps, 0)\n",
    "        selected_indices = select_min_overlap_cuboids(overlaps, sample_size)\n",
    "        crop_corners = crop_corners[selected_indices , :]\n",
    "        \n",
    "    crop_occupancy = calculate_occupancy(img_shape , crop_corners)\n",
    "         \n",
    "    crops = []\n",
    "    cboxes = []\n",
    "    \n",
    "    for fb in crop_corners :\n",
    "        nimg = img[..., fb[0]:fb[3], fb[1]:fb[4], fb[2]:fb[5]]\n",
    "        if (bbox is None) or (bbox.shape[0] == 0):\n",
    "            nbbox = np.empty((0, 6))\n",
    "        else:\n",
    "            nbbox = crop_bbox(bbox, fb, crop_size, keep_volume)\n",
    "                        \n",
    "        crops.append(nimg)\n",
    "        cboxes.append(nbbox)\n",
    "    \n",
    "    if len(crops) == 1 :\n",
    "        return crops[0] , cboxes[0] , crop_occupancy\n",
    "    \n",
    "    return crops, cboxes, crop_occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fecf3322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 ms, sys: 8.39 ms, total: 37.7 ms\n",
      "Wall time: 36.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 28.,  9., 20., 68.]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nimg, nbbox, crop_occupancy = random_crop_multi(img, bbox, crop_size ,1)\n",
    "nbbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "07fff52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.91 s, sys: 799 ms, total: 3.71 s\n",
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_size = 4\n",
    "for i in range(100):\n",
    "    nimg, nbbox, crop_occupancy = random_crop_multi(img, bbox, crop_size, sample_size)\n",
    "    if len(nbbox) < 1 :\n",
    "        print(\"Failed\")\n",
    "    for j in range(sample_size) :\n",
    "        if nimg[j].sum()==0: \n",
    "            print(\"Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cbe68",
   "metadata": {},
   "source": [
    "## RandomCrop Image\n",
    "- There can be a situation where the bounding boxes are zeros and we just crop the image randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "826c4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def random_crop_img(img, crop_size):\n",
    "    img_shape = img.shape if len(img.shape) == 3 else img.shape[1:]\n",
    "    if (np.asarray(crop_size) >= np.asarray(img_shape)).any(): img = pad3d_by_rsize(img, None, crop_size)\n",
    "    lv = np.asarray(img_shape) - np.asarray(crop_size)\n",
    "    y = [np.random.randint(i) if i>0 else 0 for i in lv]\n",
    "    z1, y1, x1, z2, y2, x2 = tuple(y) + tuple(y+np.asarray(crop_size))\n",
    "    assert (z2-z1, y2-y1, x2-x1) == tuple(crop_size), f\"crop_size is {crop_size} and got dims={y}\"\n",
    "    cimg = img[..., z1:z2, y1:y2, x1:x2]\n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "089e6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((2, 10, 20, 20))\n",
    "fc.test_eq(random_crop_img(img, (3, 10, 10)).shape[1:], (3, 10, 10))\n",
    "fc.test_eq(random_crop_img(img, (12, 10, 10)).shape[1:], (12, 10, 10))\n",
    "fc.test_eq(random_crop_img(img, (12, 4, 4)).shape[1:], (12, 4, 4))\n",
    "fc.test_eq(random_crop_img(img, (8, 4, 4)).shape[1:], (8, 4, 4))\n",
    "fc.test_eq(random_crop_img(img, (8, 24, 4)).shape[1:], (8, 24, 4))\n",
    "fc.test_eq(random_crop_img(img, (24, 24, 24)).shape[1:], (24, 24, 24))\n",
    "fc.test_eq(random_crop_img(img, (21, 21, 21)).shape[1:], (21, 21, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ac127b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((10, 20, 20))\n",
    "fc.test_eq(random_crop_img(img, (3, 10, 10)).shape, (3, 10, 10))\n",
    "fc.test_eq(random_crop_img(img, (12, 10, 10)).shape, (12, 10, 10))\n",
    "fc.test_eq(random_crop_img(img, (12, 4, 4)).shape, (12, 4, 4))\n",
    "fc.test_eq(random_crop_img(img, (8, 4, 4)).shape, (8, 4, 4))\n",
    "fc.test_eq(random_crop_img(img, (8, 24, 4)).shape, (8, 24, 4))\n",
    "fc.test_eq(random_crop_img(img, (24, 24, 24)).shape, (24, 24, 24))\n",
    "fc.test_eq(random_crop_img(img, (21, 21, 21)).shape, (21, 21, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22caf0f4",
   "metadata": {},
   "source": [
    "## RandPosCrop\n",
    "- Combining everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e23892f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class RandPosCrop(BaseT):\n",
    "    def __init__(self, crop_size: List[Tuple[int]], ignore_bbox=False, crop_strategy: str = \"v1\" , sample_size: int = 1, keep_volume: float = 0.5):\n",
    "        \"\"\"crop_size is [zyx]\"\"\"\n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "        allowed_crop_strategy = [\"v1\", \"v2\"]\n",
    "        if self.crop_strategy not in allowed_crop_strategy :\n",
    "            raise ValueError(f\"crop_strategy must be one of {allowed_crop_strategy}, got '{self.crop_strategy}' instead\")\n",
    "\n",
    "        self.crop_size_index = 0 \n",
    "\n",
    "    __repr__ = fc.basic_repr(flds=\"crop_size\")\n",
    "    \n",
    "    def apply(self, img: dict):\n",
    "        crop_size = self.crop_size[self.crop_size_index]\n",
    "        assert all([i in list(img.keys()) for i in [\"boxes\", \"images\"]])\n",
    "        fimg, bbox = img[\"images\"].copy(), img[\"boxes\"].copy()\n",
    "        img_shape = fimg.shape if len(fimg.shape) == 3 else fimg.shape[1:]\n",
    "        bbox = clip_2_img(bbox, img_shape)        \n",
    "        pimg, pbbox = pad3d_by_rsize(fimg, bbox, crop_size)\n",
    "\n",
    "        if  (~self.ignore_bbox) & (pbbox.shape[0] != 0):\n",
    "            if self.crop_strategy == \"v1\" : \n",
    "                cimg, cbbox, _ = random_crop(pimg, pbbox, crop_size, self.sample_size, self.keep_volume)\n",
    "            if self.crop_strategy == \"v2\" : \n",
    "                cimg , cbbox, _ = random_crop_multi(pimg, pbbox, crop_size, self.sample_size, self.keep_volume)\n",
    "        else:\n",
    "            cimg = random_crop_img(pimg, crop_size)#no bounding boxes.\n",
    "            cbbox = pbbox\n",
    "        \n",
    "        nimg = {\"images\": cimg, \"boxes\": cbbox}\n",
    "        for i in img.keys(): \n",
    "            if i not in [\"images\", \"boxes\"]: nimg[i] = img[i]\n",
    "        return nimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b8b72",
   "metadata": {},
   "source": [
    "> ignore bbox even when they are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7e641ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2 1 4 3 2]\n",
      " [1 1 1 2 2 2]] (12, 13, 14) [[3 2 1 4 3 2]\n",
      " [1 1 1 2 2 2]] (6, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "tr = RandPosCrop(crop_size=(6, 6, 6), ignore_bbox=True)\n",
    "img = {}\n",
    "img[\"images\"] = np.ones((12, 13, 14))\n",
    "img[\"boxes\"] = np.asarray([[3, 2, 1, 4, 3, 2], [1, 1, 1, 2, 2, 2]])\n",
    "trimg = tr(img)\n",
    "print(img[\"boxes\"], img[\"images\"].shape, trimg[\"boxes\"], trimg[\"images\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f168d8",
   "metadata": {},
   "source": [
    "> test when there is no bbox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4131485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] (12, 13, 14) [] (6, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "tr = RandPosCrop(crop_size=(6, 6, 6))\n",
    "img = {}\n",
    "img[\"images\"] = np.ones((12, 13, 14))\n",
    "img[\"boxes\"] = np.empty((0, 6))\n",
    "trimg = tr(img)\n",
    "print(img[\"boxes\"], img[\"images\"].shape, trimg[\"boxes\"], trimg[\"images\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d6540",
   "metadata": {},
   "source": [
    " >test when have to sample more than 1 cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c49cf34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2 1 4 3 2]\n",
      " [1 1 1 2 2 2]] (12, 13, 14) [array([[3., 1., 1., 4., 2., 2.],\n",
      "       [1., 0., 1., 2., 1., 2.]]), array([[3., 2., 0., 4., 3., 1.],\n",
      "       [1., 1., 0., 2., 2., 1.]])] 2\n"
     ]
    }
   ],
   "source": [
    "tr = RandPosCrop(crop_size=(6, 6, 6), sample_size= 2)\n",
    "img = {}\n",
    "img[\"images\"] = np.ones((12, 13, 14))\n",
    "img[\"boxes\"] = np.asarray([[3, 2, 1, 4, 3, 2], [1, 1, 1, 2, 2, 2]])\n",
    "trimg = tr(img)\n",
    "print(img[\"boxes\"], img[\"images\"].shape, trimg[\"boxes\"], len(trimg[\"images\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b5e7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voxdet.utils import image_grid, thumbnail\n",
    "def draw_boxes_on_image(image, boxes):\n",
    "    drawn_image = np.copy(image)\n",
    "    for box in boxes:\n",
    "        zmin, ymin, xmin = np.floor(box[:3]).astype(int)\n",
    "        zmax, ymax, xmax = np.ceil(box[3:]).astype(int)-1\n",
    "\n",
    "        drawn_image[zmin, ymin:ymax, xmin:xmax] = 1\n",
    "        drawn_image[zmax, ymin:ymax, xmin:xmax] = 1\n",
    "        drawn_image[zmin:zmax, ymin, xmin:xmax] = 1\n",
    "        drawn_image[zmin:zmax, ymax, xmin:xmax] = 1\n",
    "        drawn_image[zmin:zmax, ymin:ymax, xmin] = 1\n",
    "        drawn_image[zmin:zmax, ymin:ymax, xmax] = 1\n",
    "        \n",
    "    return drawn_image\n",
    "\n",
    "def my_vis(img:np.ndarray, slices:np.ndarray):\n",
    "    img = [thumbnail(img[i], 192) for i in slices]\n",
    "    return image_grid(img, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from safetensors.numpy import load_file\n",
    "\n",
    "img = load_file(\"../../resources/1.3.6.1.4.1.14519.5.2.1.6279.6001.249404938669582150398726875826.safetensors\")\n",
    "img.keys(), img[\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Norm3d(a_min = -1024.0, a_max = 300.0, b_min = 0.0, b_max= 1.0, clip= True)\n",
    "nimg = norm(img)\n",
    "\n",
    "pin = PadIfNeeded(sd=32)\n",
    "pimg = pin(nimg)\n",
    "\n",
    "rpc = RandPosCrop(crop_size=(96, 192, 192))\n",
    "cimg = rpc(pimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6554df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], shape=(0, 6), dtype=float64),\n",
       " array([], shape=(0, 6), dtype=float64))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[\"boxes\"], cimg[\"boxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac45354",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodule_slices = np.arange(math.floor(cimg[\"boxes\"][0][0]), math.ceil(cimg[\"boxes\"][0][3]))\n",
    "my_vis(draw_boxes_on_image(cimg[\"images\"].copy(), cimg[\"boxes\"].copy())*255, nodule_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8e2366d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ApplyWindowsChannelWise(BaseT):\n",
    "    from qct_utils.cv_ops.windowing import Window\n",
    "    from typing import List\n",
    "    def __init__(self, windows: List[Window], renorm: bool = True):\n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "    __repr__ = fc.basic_repr(flds=\"windows,renorm\")\n",
    "\n",
    "    def apply(self, img: dict):\n",
    "        assert \"images\" in img.keys(), \"images key is not present\"\n",
    "        fimg = img[\"images\"]\n",
    "        nimg = {}\n",
    "        nimg[\"images\"] = self.apply_image(fimg)\n",
    "        for i in img.keys(): \n",
    "            if i not in nimg.keys(): nimg[i] = img[i]\n",
    "        return nimg \n",
    "    \n",
    "    def apply_image(self, img):\n",
    "        from qct_utils.cv_ops.windowing import stack_multiple_windows\n",
    "        img = stack_multiple_windows(img , self.windows)\n",
    "        if self.renorm:\n",
    "            img = 2*img - 1 # renormalize the image from [-1, 1]\n",
    "        return img\n",
    "    \n",
    "    def reverse_apply(self, img: dict): return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6908679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ToNumpy(BaseT):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, img: dict):\n",
    "        import torch\n",
    "        assert \"images\" in img.keys(), \"images key is not present\"\n",
    "        for i in img.keys(): \n",
    "            if isinstance(img[i] ,torch.Tensor) :\n",
    "                img[i] = img[i].numpy()\n",
    "        return img\n",
    "\n",
    "    def reverse_apply(self, img: dict): return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fbdfdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = \"/home/users/shubham.kumar/cache/lidc_cache/1.3.6.1.4.1.14519.5.2.1.6279.6001.991510467496831189331312523884_left.safetensors\"\n",
    "\n",
    "\n",
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(pp, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be86b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qct_utils.cv_ops.windowing import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7db3cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [Window(ww = 400, wl = -500 ) , Window(ww = 1500 , wl = -500), Window(ww = 800,wl = -800)]\n",
    "windows_channel = ApplyWindowsChannelWise(windows = windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e3c1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qct_utils.ctvis.viewer import plot_scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c469143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = tensors[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ea7362be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = windows_channel.apply(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9be067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numpy = ToNumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc7333c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = to_numpy.apply(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d4d58c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 96, 336, 274)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b5e04e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = PadIfNeeded(sd = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "425d565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pin.apply(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8233de5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 96, 352, 288)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1fc244b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voxdet.tfsm.mip import MIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fbc1e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MIP(num_slices = 5, stride= 1, mode= \"max\", return_stacked_img= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5a1bb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = mp.apply(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8cc3f0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 96, 352, 288)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9704dc4ef4405580d46f2c7c486425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(RadioButtons(description='Slice plane selection:', index=2, options=('y-z', 'z-x', 'x-y'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_scans([pp[\"images\"][2]],[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
