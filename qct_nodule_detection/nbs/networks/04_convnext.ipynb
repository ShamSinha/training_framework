{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07084f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp networks/convnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3114abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import fastcore.all as fc\n",
    "\n",
    "from voxdet.networks.fpn import BackbonewithFPN3D\n",
    "from voxdet.networks.res_se_net import conv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db336fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf66630",
   "metadata": {},
   "source": [
    "## ConvNext \n",
    "They have made several changes to ResNet to make it as roboust as possible to `Swin-Transformers`. We will explore some of the changes here. \n",
    "- Change Stage Compute ratio: For ResNet50, we have (3, 4, 6, 3). They changed it to (3, 3, 9, 3). For ResNet10, lets change it to `(1, 3, 3, 1)`\n",
    "- stem cell 4x4 stride 4 conv layer, 96 channels - 0.1% improvement\n",
    "- They have added inverted blocks with depthwise conv with large kernel size and 1x1 conv: \n",
    "\n",
    "```\n",
    "d3x3, 96x96\n",
    "1x1, 96x384\n",
    "1x1, 384x96\n",
    "```\n",
    "- use GELU instead of RELU, also GELU will be only between two 1x1 blocks \n",
    "- we will use LN instead of BN, Also we will use only few batch-norm layers: before 1x1 layer. \n",
    "- instead of downsampling included in the stages, we will include this as a separate step after each stage. Use 2x2 conv with stride 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c0e008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 192, 192])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.zeros((1, 1, 96, 192, 192))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64235848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LayerNorm3d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '3D' spatial NCDHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 4, 1) #NCDHW -> NDHWC\n",
    "        #(0, 2, 3, 1) -> NCHW -> NHWC \n",
    "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        x = x.permute(0, 4, 1, 2, 3) # NDHWC -> NCDHW\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4551ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    copied from https://github.com/rwightman/pytorch-image-models/blob/7d9e321b761a673000af312ad21ef1dec491b1e9/timm/layers/drop.py#L137\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "    __repr__ = fc.basic_repr(\"drop_prob, scale_by_keep\")\n",
    "    def forward(self, x):return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0d9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DropPath(0.1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a025023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.2 ms, sys: 0 ns, total: 95.2 ms\n",
      "Wall time: 32.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 192, 192])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time x(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26fc5d",
   "metadata": {},
   "source": [
    "### Stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70eb6039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv3d(1, 96, kernel_size=(4, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0), bias=False)\n",
       "  (1): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = conv3d(1, 96, ks=4, stride=(2, 4, 4), norm=LayerNorm3d, padding=(1, 0, 0))\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeef05f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 48, 48, 48])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = stem(img)\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742013ec",
   "metadata": {},
   "source": [
    "### Depthwise conv \n",
    "`!pip install fvcore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c139e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dconv = nn.Conv3d(96, 96, stride=2, kernel_size=4, groups=96)\n",
    "nconv = nn.Conv3d(96, 96, stride=2, kernel_size=4)\n",
    "tconv = nn.Conv3d(64, 64, stride=2, kernel_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a50b030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74754048, 7176388608, 3189506048, 96.0, 42.666666666666664)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis\n",
    "flops = FlopCountAnalysis(dconv, outs)\n",
    "flops2 = FlopCountAnalysis(nconv, outs)\n",
    "flops3 = FlopCountAnalysis(tconv, torch.zeros((1, 64, 48, 48, 48)))\n",
    "flops.total(), flops2.total(), flops3.total(), flops2.total()/flops.total(), flops3.total()/flops.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90474a36",
   "metadata": {},
   "source": [
    "### ConvNextBlock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab1943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm3d(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.pwconv1 = nn.Conv3d(dim, 4 * dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv3d(4 * dim, dim, kernel_size=1)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        x = inputs + self.drop_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01269b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextBlock(\n",
       "  (dwconv): Conv3d(96, 96, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=96)\n",
       "  (norm): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  (pwconv1): Conv3d(96, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (act): GELU(approximate=none)\n",
       "  (pwconv2): Conv3d(384, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (drop_path): Identity()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = ConvNextBlock(96)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52656c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 7.85 s, total: 30.1 s\n",
      "Wall time: 768 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 48, 48, 48])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time block(outs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0be02",
   "metadata": {},
   "source": [
    "### ConvNextStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1cb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ConvNextStage(nn.Module):\n",
    "    def __init__(self, dims, layers, dp_rates=0, normalize=None): \n",
    "        fc.store_attr()\n",
    "        super().__init__()\n",
    "        if not isinstance(dp_rates, list): dp_rates = [x.item() for x in torch.linspace(0, dp_rates, layers)]  \n",
    "        for i in range(self.layers): \n",
    "            setattr(self, f\"layer{i}\", ConvNextBlock(dims, drop_path=dp_rates[i]))\n",
    "            if self.normalize is not None: setattr(self, f\"norm{i}\", self.normalize(dims))\n",
    "            \n",
    "    def forward(self, x): \n",
    "        for i in range(self.layers): x = getattr(self, f\"layer{i}\")(x)\n",
    "        if self.normalize is not None: x = getattr(self, f\"norm{i}\")(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9778c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextStage(\n",
       "  (layer0): ConvNextBlock(\n",
       "    (dwconv): Conv3d(96, 96, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=96)\n",
       "    (norm): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    (pwconv1): Conv3d(96, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (act): GELU(approximate=none)\n",
       "    (pwconv2): Conv3d(384, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (drop_path): Identity()\n",
       "  )\n",
       "  (norm0): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  (layer1): ConvNextBlock(\n",
       "    (dwconv): Conv3d(96, 96, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=96)\n",
       "    (norm): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       "    (pwconv1): Conv3d(96, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (act): GELU(approximate=none)\n",
       "    (pwconv2): Conv3d(384, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (drop_path): Identity()\n",
       "  )\n",
       "  (norm1): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = ConvNextStage(96, 2, dp_rates=0, normalize=LayerNorm3d)\n",
    "stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1da9c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 s, sys: 16.2 s, total: 1min 5s\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 48, 48, 48])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time stage(outs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9578d3",
   "metadata": {},
   "source": [
    "### Adding ConvNext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d31a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0058823530562222, 0.0117647061124444]\n",
      "[0.01764705963432789, 0.0235294122248888, 0.029411764815449715]\n",
      "[0.03529411926865578, 0.04117647185921669, 0.0470588244497776, 0.052941177040338516, 0.05882352963089943, 0.06470588594675064, 0.07058823853731155, 0.07647059112787247, 0.08235294371843338]\n",
      "[0.0882352963089943, 0.0941176488995552, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "depths = [3, 3, 9, 3]\n",
    "dp = [x.item() for x in torch.linspace(0, 0.1, sum(depths))]\n",
    "for i in range(len(depths)):\n",
    "    print(dp[sum(depths[:i]): sum(depths[:i+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17d07567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ConvNext(nn.Module):\n",
    "    def __init__(self, ic=1, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], c1_ks=(4, 4, 4), c1_s=(2, 4, 4),\n",
    "                 drop_path_rate=0.):\n",
    "        fc.store_attr()\n",
    "        super().__init__()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, self.drop_path_rate, sum(depths))]\n",
    "        pad = 0 if c1_s[0] == c1_s[1] else 1 \n",
    "        for i in range(len(self.depths)):  \n",
    "            if i == 0:  \n",
    "                setattr(self, f\"base\", conv3d(ic, dims[0], ks=c1_ks, stride=c1_s, norm=LayerNorm3d, padding=(pad, 0, 0)))\n",
    "            else:\n",
    "                setattr(self, f\"downsample{i}\", nn.Conv3d(dims[i-1], dims[i], kernel_size=2, stride=2, padding=(0, 0, 0)))\n",
    "            \n",
    "            dp_rates_layer = dp_rates[sum(depths[:i]): sum(depths[:i+1])]\n",
    "            setattr(self, f\"stage{i+1}\", ConvNextStage(dims[i], layers=depths[i], dp_rates=dp_rates_layer))            \n",
    "        \n",
    "    \n",
    "    def forward(self, x): \n",
    "        out = x \n",
    "        for i in range(len(self.dims)):\n",
    "            if i==0: out = self.base(out)\n",
    "            else: out = getattr(self, f\"downsample{i}\")(out)\n",
    "            out = getattr(self, f\"stage{i+1}\")(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3200d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNext(\n",
       "  (base): Sequential(\n",
       "    (0): Conv3d(1, 96, kernel_size=(4, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0), bias=False)\n",
       "    (1): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stage1): ConvNextStage(\n",
       "    (layer0): ConvNextBlock(\n",
       "      (dwconv): Conv3d(96, 96, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=96)\n",
       "      (norm): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      (pwconv1): Conv3d(96, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (act): GELU(approximate=none)\n",
       "      (pwconv2): Conv3d(384, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (downsample1): Conv3d(96, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "  (stage2): ConvNextStage(\n",
       "    (layer0): ConvNextBlock(\n",
       "      (dwconv): Conv3d(192, 192, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=192)\n",
       "      (norm): LayerNorm3d((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (pwconv1): Conv3d(192, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (act): GELU(approximate=none)\n",
       "      (pwconv2): Conv3d(768, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (downsample2): Conv3d(192, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "  (stage3): ConvNextStage(\n",
       "    (layer0): ConvNextBlock(\n",
       "      (dwconv): Conv3d(384, 384, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=384)\n",
       "      (norm): LayerNorm3d((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (pwconv1): Conv3d(384, 1536, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (act): GELU(approximate=none)\n",
       "      (pwconv2): Conv3d(1536, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (downsample3): Conv3d(384, 768, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "  (stage4): ConvNextStage(\n",
       "    (layer0): ConvNextBlock(\n",
       "      (dwconv): Conv3d(768, 768, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), groups=768)\n",
       "      (norm): LayerNorm3d((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pwconv1): Conv3d(768, 3072, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (act): GELU(approximate=none)\n",
       "      (pwconv2): Conv3d(3072, 768, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c10 = ConvNext(ic=1, depths=(1, 1, 1, 1), dims=[96, 192, 384, 768])\n",
    "c10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff762c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.4 s, sys: 8.47 s, total: 49.9 s\n",
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 6, 6, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time c10(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0fd00",
   "metadata": {},
   "source": [
    "### Is item getter working in ths case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a13e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1149a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_layers = [1, 2, 3, 4]\n",
    "return_layers = {f\"stage{k}\": str(v) for v, k in enumerate(returned_layers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c3915b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = torchvision.models._utils.IntermediateLayerGetter(c10, return_layers=return_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59158f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 s, sys: 6.44 s, total: 44.5 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%time outs = body(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a510131e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', torch.Size([1, 96, 48, 48, 48])),\n",
       " ('1', torch.Size([1, 192, 24, 24, 24])),\n",
       " ('2', torch.Size([1, 384, 12, 12, 12])),\n",
       " ('3', torch.Size([1, 768, 6, 6, 6]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.shape) for k, v in outs.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc82ed3",
   "metadata": {},
   "source": [
    "## ConvNext 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b73a9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def convnext10(ic, dims=[96, 192, 384, 768], c1_ks=(4, 4, 4), c1_s=(2, 4, 4), drop_path_rate=0.):\n",
    "    c10 = ConvNext(ic=ic, depths=(1, 1, 1, 1), dims=dims, c1_ks=c1_ks, c1_s=c1_s, drop_path_rate=drop_path_rate)\n",
    "    return c10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9245b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def convnext18(ic, dims=[96, 192, 384, 768], c1_ks=(4, 4, 4), c1_s=(2, 4, 4), drop_path_rate=0.):\n",
    "    c18 = ConvNext(ic=ic, depths=(2, 2, 2, 2), dims=dims, c1_ks=c1_ks, c1_s=c1_s, drop_path_rate=drop_path_rate)\n",
    "    return c18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed1b01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def convnext50(ic, dims=[96, 192, 384, 768], c1_ks=(4, 4, 4), c1_s=(2, 4, 4), drop_path_rate=0.):\n",
    "    c50 = ConvNext(ic=ic, depths=(3, 3, 9, 3), dims=dims, c1_ks=c1_ks, c1_s=c1_s, drop_path_rate=drop_path_rate)\n",
    "    return c50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba0f69",
   "metadata": {},
   "source": [
    "## with FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a51a42b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96, 192, 384, 768]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c10.dims[-1]//8 * 2 ** (i - 1) for i in [1, 2, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9373a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def convnext_fpn3d_feature_extractor(backbone, out_channels=256, returned_layers=[1, 2, 3], extra_blocks:bool=False):\n",
    "    in_channels_stage2 = backbone.dims[-1] // 8\n",
    "    in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
    "    return_layers = {f\"stage{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
    "    return BackbonewithFPN3D(backbone, return_layers, in_channels_list, out_channels, extra_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b497ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn = convnext_fpn3d_feature_extractor(c10, extra_blocks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eed7d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv3d(1, 96, kernel_size=(4, 4, 4), stride=(2, 4, 4), padding=(1, 0, 0), bias=False)\n",
       "  (1): LayerNorm3d((96,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpn.body.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b74bc13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 32, 32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(2 * s * 2 ** max([1, 2]) for s in fpn.body.base[0].stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time out = fpn(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v.shape) for k, v in out.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a8380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qct_deep)",
   "language": "python",
   "name": "qct_deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
