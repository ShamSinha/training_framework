# @package _global_

# to execute this experiment run:
# python src/train.py experiment=lung_lobe_segmentation

train: True  # if true train the model
test: True # if true test the model
# if ckpt_path is not none and train is true: load the initial weight from the ckpt_path
# if ckpt_path is not none and train is false: test the model mentioned in ckpt_path
skip_clearml: True # in case it's a test experiment
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-12_23-59-33/checkpoints/epoch=50_step=1683_val_dice=0.87.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-23_18-28-07/checkpoints/epoch=25_step=338_val_dice=0.66.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-24_16-11-36/checkpoints/epoch=30_step=426_val_dice=0.85.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-25_03-44-35/checkpoints/epoch=36_step=570_val_dice=0.87.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-25_11-45-03/checkpoints/epoch=39_step=636_val_dice=0.89.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-26_19-06-35/checkpoints/epoch=50_step=1131_val_dice=0.90.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-27_04-33-06/checkpoints/epoch=63_step=1755_val_dice=0.91.ckpt 
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-04-27_18-37-34/checkpoints/epoch=65_step=1805_val_dice=0.91.ckpt

# only lidc data
# trained on only keep lung
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-05_11-58-02/checkpoints/epoch=48_step=49_val_dice=0.69.ckpt   
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-05_16-35-58/checkpoints/epoch=78_step=79_val_dice=0.91.ckpt 

# trained using above ckpt and without crop
# ckpt_path : /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-06_14-34-33/checkpoints/epoch=109_step=110_val_dice=0.86.ckpt   
#ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-06_15-55-23/checkpoints/epoch=139_step=140_val_dice=0.93.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-07_05-13-39/checkpoints/epoch=146_step=168_val_dice=0.89.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-07_22-04-38/checkpoints/epoch=186_step=328_val_dice=0.92.ckpt

# Replaced Custom LungB with Lung
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-11_19-16-42/checkpoints/epoch=199_step=406_val_dice=0.93.ckpt #production model

# Added Vessel12 dataset
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-26_14-23-28/checkpoints/epoch=216_step=491_val_dice=0.93.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-27_13-01-35/checkpoints/epoch=224_step=531_val_dice=0.94.ckpt
#ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-28_15-33-33/checkpoints/epoch=244_step=631_val_dice=0.93.ckpt
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-29_11-38-22/checkpoints/epoch=245_step=636_val_dice=0.94.ckpt 
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-29_15-58-19/checkpoints/epoch=252_step=643_val_dice=0.96.ckpt
#ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-29_16-19-57/checkpoints/epoch=256_step=663_val_dice=0.94.ckpt

# Added rad_chest_v1 dataset
#ckpt_path : /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-30_11-44-16/checkpoints/epoch=263_step=705_val_dice=0.95.ckpt #production model
ckpt_path : /home/users/shubham.kumar/projects/lung_lobe_segmentation/lung_lobe_seg_4_decoder_blocks.ckpt  #It is same model as above but 1 decoder block is removed.

# Added fused lobe dataset
# ckpt_path: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/lung_lobe_segmentation_train/runs/2023-05-24_17-11-37/checkpoints/epoch=215_step=486_val_dice=0.92.ckpt


defaults:
  - override /datamodule: lung_lobe_segmentation_datamodule.yaml # Ignore everything mentioned in default train.yaml. Load none dict from /datamodule
  - override /model: lung_lobe_segmentation
   # Load the /model/nodule_segmentation.yaml config group
  - override /model/net/segmentation: timm_models # Update the above loaded with new config group. /model/net =  /model/net/resunet. This can be changed from commandline too /model/net=attn_unet
  - _self_ # After loading the above merge with below
  # Alternatively instead of writing everything here. we can write yaml files within trainer or callbacks or
  # - override /trainer: nodule_segmentation.yaml
  # - override /paths: nodule_segmentation.yaml

# all parameters below will be merged with parameters from default configurations set above or mentioned in train.yaml
# this allows you to overwrite only specified parameters

task_name: lung_lobe_segmentation_train # Everything is stored under ${paths.log_dir}/{$task_name}/runs

# Clearml project is created with ${clearml_project} and ${clearml_task_name} is the name present in experiments.qure.ai
clearml_project: ${task_name}
clearml_task_name: lung_lobe_data_included_rad_chest_v2_decoder_block_4 # Keeping it mandatory for storing names

# tags to help you identify your experiments
# you can overwrite this in experiment configs
# overwrite from command line with `python train.py tags="[first_tag, second_tag]"`
# appending lists from command line is currently not supported :(
# https://github.com/facebookresearch/hydra/issues/1547
# tags: ["dev"]
tags: ["test"]

seed: 12345

## Data Module
# datamodule:
# Add any key you want to overwrite.

## Lightning trainer
trainer: # Merge with default trainer mentioned above
  default_root_dir: ${paths.output_dir}
  accelerator: gpu
  devices: [2]
  max_epochs: 340
  min_epochs: 300
  gradient_clip_val: 0.5
  gradient_clip_algorithm: norm
  accumulate_grad_batches: 16
  precision: 16-mixed
  num_sanity_val_steps: 0
  sync_batchnorm: True
  # strategy: ddp
  # strategy: ddp_find_unused_parameters_true
 
## Lighning callbacks
callbacks: # Merged with defaults mentioned in callbacks/default.yaml
  model_checkpoint: # This is the name of the metric. It is used to save the checkpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: "{epoch}_{step}_{val_dice:.2f}"
    monitor: "val_dice" # using val/dice has some weird behaviour while saving checkpoint. It is stored as dir val/dice
    mode: "max"
    save_last: True
    auto_insert_metric_name: True
    save_top_k: 2
    verbose: true
    
  pred_logger:
    num_samples: 4
    epoch_interval: 1
    log_gt_label: False

  # SegSubAnalysis:
  #   subgroups_keys: [z_spacing, volume, Texture, Calcification, Spiculation]

  # early_stopping:
  #   monitor: "val_loss"
  #   patience: 30
  #   mode: "min"

  model_summary:
    max_depth: 3

  stochastic_weight_averaging:
    swa_epoch_start: 0.67 # Make this 1 to turn this off.
    swa_lrs: 0.05
    annealing_epochs: 20

## Defining paths
paths:
  # path to root directory
  # this requires PROJECT_ROOT environment variable to exist
  # PROJECT_ROOT is inferred and set by pyrootutils package in `train.py` and `eval.py`
  root_dir: ${oc.env:PROJECT_ROOT}

  # path to data directory, No data directory is being used.
  data_dir: null

  # path to logging directory
  log_dir: /data_nas5/qer/shubham/lung_seg_checkpoints/model_checkpoints/training/

  # path to output directory, created dynamically by hydra
  # path generation pattern is specified in `configs/hydra/default.yaml`
  # use it to store all files generated during the run, like ckpts and metrics
  # defaults to this log_dir/task_name
  output_dir: ${hydra:runtime.output_dir}

  # path to working directory
  work_dir: ${hydra:runtime.cwd}


